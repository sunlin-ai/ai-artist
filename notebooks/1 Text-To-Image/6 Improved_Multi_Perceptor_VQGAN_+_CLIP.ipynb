{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improved Multi Perceptor VQGAN + CLIP [Public]",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "NEAdjUn3RcNm",
        "IbrDcvJNNLO0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karen-pal/notebook/blob/main/Improved_Multi_Perceptor_VQGAN_%2B_CLIP_%5BPublic%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL3qnIS2qC9K"
      },
      "source": [
        "# Improved Multi-Perceptor VQGAN + CLIP (v.4.2022.05.05)\n",
        "by [@remi_durant](https://twitter.com/remi_durant) \n",
        "\n",
        "\n",
        "\n",
        "*Lots has been drawn from or inspired by other colabs, chief among them is [@jbusted1](https://twitter.com/jbusted1)'s MSE regularized VQGAN + Clip, and [@RiversHaveWings](https://twitter.com/RiversHaveWings) VQGAN + Clip with Z+Quantize. Standing on the shoulders of giants.*\n",
        "\n",
        "#### üéÅ Improvements\n",
        "- UI Ease-of-Use Revamp üòÄ\n",
        "- New simple prompt templating system -- when writing a prompt you can include `{red|blue}` to have it randomly pick one of the options\n",
        "- Added advanced section with multiple new tools for power users \n",
        " - Madlib Style Prompt Templates\n",
        " - Batch Runs\n",
        " - Noise Burst\n",
        " - Zoom & Rotate Animation\n",
        "- Add new cut method which takes larger cuts, letterboxing as needed, to help with overall cohesion\n",
        "- Weigh losses of each cut by their area, so larger cuts have more influence over the generation which also helps with overall cohesion\n",
        "- New option to save run paramters into a json file for future reference\n",
        "\n",
        "<details>\n",
        "<summary>Continued...</summary>\n",
        "\n",
        "- Multi-clip mode sends the same cutouts to whatever clip models you want. If they have different perceptor resolutions, the cuts are generated at each required size, replaying the same augments across both scales\n",
        "- Alternate random noise generation options to use as start point (perlin, pyramid, or vqgan random z tokens)\n",
        "- MSE Loss doesn't apply if you have no init_image until after reaching the first epoch.\n",
        "- MSE epoch targets z.tensor, not z.average, to allow for more creativity\n",
        "- Grayscale augment added for better structure\n",
        "- Padding fix for perspective and affine augments to not always be black barred\n",
        "- Automatic disable of cudnn for A100\n",
        "</details>\n",
        "\n",
        "<details><summary>Old Versions</summary>\n",
        "\n",
        "If you want to check out older versions of this notebook, here they are!\n",
        "- [Version 3 (v.3.2022.03.23)](https://colab.research.google.com/drive/1b6Se2FS8q3XcwkQiVj4nnVK8mdUae3Xi?usp=sharing)\n",
        "- [Version 2 (v.2.2021.11.21)](https://colab.research.google.com/drive/1rfTXQ9BcJnsXzxYMfkq53cz9mz_fjR8v?usp=sharing)\n",
        "- [Version 1 (v.1.2021.11.13)](https://colab.research.google.com/drive/1gHvfL8X2p0Eex3i8WjSKei9ZG9oawH6L?usp=sharing)\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "Be sure to check out my [Artist Studies](https://remidurant.com/artists/) project as well, to help you find good artist names to add to your prompts. \n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=remi_multiclipvqgan3)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93QxgHDB6jkr",
        "cellView": "form"
      },
      "source": [
        "#@title First check what GPU you got and make sure it's a good one. \n",
        "#@markdown - Tier List: (K80 < T4 < P100 < V100 < A100)\n",
        "from subprocess import getoutput\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEAdjUn3RcNm"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdC-w8aCQPAQ",
        "cellView": "form"
      },
      "source": [
        "#@title memory footprint support libraries/code\n",
        "\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZnX4Z6ScpKh",
        "cellView": "form"
      },
      "source": [
        "#@title Print GPU details\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn‚Äôt guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNIJyIYLQRsH",
        "cellView": "form"
      },
      "source": [
        "#@title Install Dependencies\n",
        "\n",
        "# Fix for A100 issues\n",
        "#!pip install tensorflow==1.15.2\n",
        "\n",
        "# Install normal dependencies\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtLgtENkRDk0",
        "cellView": "form"
      },
      "source": [
        "#@title Load libraries and variables\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import os.path\n",
        "from os import path\n",
        "from urllib.request import Request, urlopen\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia\n",
        "import kornia.augmentation as K\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "import random\n",
        "import gc\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "from base64 import b64encode\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.set_printoptions( sci_mode=False )\n",
        "\n",
        "def noise_gen(shape, octaves=5):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    max_octaves = min(octaves, math.log(h)/math.log(2), math.log(w)/math.log(2))\n",
        "    for i in reversed(range(max_octaves)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)#(input / input.norm(dim=-1, keepdim=True)).unsqueeze(1)# \n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)#(self.embed / self.embed.norm(dim=-1, keepdim=True)).unsqueeze(0)#\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        \n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
        "          \n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "  \n",
        "import io\n",
        "import base64\n",
        "def image_to_data_url(img, ext):  \n",
        "    img_byte_arr = io.BytesIO()\n",
        "    img.save(img_byte_arr, format=ext)\n",
        "    img_byte_arr = img_byte_arr.getvalue()\n",
        "    # ext = filename.split('.')[-1]\n",
        "    prefix = f'data:image/{ext};base64,'\n",
        "    return prefix + base64.b64encode(img_byte_arr).decode('utf-8')\n",
        " \n",
        "\n",
        "def update_random( seed, purpose ):\n",
        "  if seed == -1:\n",
        "    seed = random.seed()\n",
        "    seed = random.randrange(1,99999)\n",
        "    \n",
        "  print( f'Using seed {seed} for {purpose}')\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  return seed\n",
        "\n",
        "def lerp(a,b,alpha):\n",
        "    return (b-a)*alpha + a\n",
        "    \n",
        "def clear_memory():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg715v3EmEUZ",
        "cellView": "form"
      },
      "source": [
        "#@title Setup for A100\n",
        "if gpu.name.startswith('A100'):\n",
        "  torch.backends.cudnn.enabled = False\n",
        "  print('Finished setup for A100')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MakeCuts class with debug functionality\n",
        "cut_sizes_should_use_large_dimension = False #@param {type:'boolean'}\n",
        "cut_size_mode = \"trunc_norm\" #@param [ \"normal\", \"trunc_norm\", \"cut_pow\", \"cut_map\"]\n",
        "cut_oversample = 0#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "std_factor = 1.5#@param {type:'number'}\n",
        "cut_map_weight = 0.66#@param {type:'number'}\n",
        "cut_map_size_weights = [0.6, 0.4, 0.2, 0.12] #@param\n",
        "#@markdown ----\n",
        "\n",
        "class MakeCutouts3(nn.Module):\n",
        "    def __init__(self, min_cut, cutn, oversample=0., std_factor=1.5, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cutn = cutn\n",
        "        \n",
        "        self.oversample = oversample\n",
        "        self.min_cut = torch.tensor([min_cut])\n",
        "        self.std_factor = std_factor\n",
        "\n",
        "        self.cut_pow = cut_pow\n",
        "        self.cut_map_weight = cut_map_weight\n",
        "        \n",
        "    def simulate(self,input,runs=100):\n",
        "\n",
        "        history = []\n",
        "        for _ in range(runs):\n",
        "            sizes, offsets, extents, weights = self.pick_cuts(input)\n",
        "            history.append(sizes.squeeze())\n",
        "        history = torch.cat( history, dim=0 )\n",
        "\n",
        "        heat = torch.zeros(1,*input.shape[2:4])\n",
        "        weights = weights / weights.sum()\n",
        "        offsets = offsets.clamp_min(0)\n",
        "        for i in range(self.cutn):\n",
        "          heat[...,offsets[i,0]:extents[i,0],offsets[i,1]:extents[i,1]] += 1 / self.cutn\n",
        "\n",
        "        heat = np.pi * 1.6 - heat * ( 1.3 * np.pi )\n",
        "        heat = kornia.color.hsv_to_rgb( torch.stack( (heat, torch.ones_like(heat), torch.ones_like(heat)*1), dim=1))\n",
        "\n",
        "        return history, heat\n",
        "\n",
        "    def pick_sizes(self,dims,cutn,mode,allow_lb=False):\n",
        "\n",
        "        if cut_sizes_should_use_large_dimension:\n",
        "            max_size = dims.max().unsqueeze(0)\n",
        "            min_size = torch.min( dims.min(), self.min_cut).unsqueeze(0)\n",
        "\n",
        "            rand_size_mean = ( dims.prod() * self.min_cut ) ** ( 1/3. )\n",
        "            rand_size_std = min( rand_size_mean - min_size, max_size - rand_size_mean ) * self.std_factor\n",
        "        else:\n",
        "            max_size = torch.max( dims.min(), self.min_cut )[None]\n",
        "            min_size = torch.min( dims.min(), self.min_cut )[None]\n",
        "\n",
        "            rand_size_mean = ( min_size * max_size ) ** (1/2.) \n",
        "            rand_size_std = min( abs(rand_size_mean - min_size), abs(max_size - rand_size_mean) ) * self.std_factor\n",
        "\n",
        "        # use normal distribution... \n",
        "        if mode == 'normal':\n",
        "            sizes = torch.FloatTensor(cutn,1).normal_( rand_size_mean.squeeze(), rand_size_std.squeeze() )\n",
        "            sizes = sizes.clip( min_size, max_size ).int()\n",
        "\n",
        "        # truncated normal distribution\n",
        "        elif mode == 'trunc_norm':\n",
        "            sizes = torch.nn.init.trunc_normal_(torch.FloatTensor(cutn,1), mean=rand_size_mean, std=rand_size_std, a=min_size, b=max_size ).int()\n",
        "\n",
        "        # use cut pow...\n",
        "        elif mode == 'cut_pow':\n",
        "            sizes = torch.lerp( min_size.float(), max_size.float(), torch.rand(cutn,1) ** self.cut_pow ).int()\n",
        "        \n",
        "        elif mode == 'cut_map':\n",
        "            \n",
        "            sizemap = torch.tensor( [min_size, max_size, dims.max(), dims.prod() ** 0.5 ]).int()\n",
        "            sizemap = sizemap.sort().values\n",
        "            sizes = torch.tensor( list( torch.utils.data.WeightedRandomSampler( \n",
        "                cut_map_size_weights, cutn, replacement=True) ))\n",
        "            sizes = sizemap[sizes[:,None]]\n",
        "        \n",
        "        return sizes\n",
        "\n",
        "    def pick_cuts(self,input):\n",
        "        dims = torch.tensor(input.shape[2:4]).unsqueeze(0)\n",
        "        hdims = (dims/2).int()\n",
        "\n",
        "        num_cut_maps = int( self.cutn * self.cut_map_weight )\n",
        "        num_cuts = self.cutn - num_cut_maps\n",
        "\n",
        "        sizes = []\n",
        "\n",
        "        if num_cuts > 0:\n",
        "            sizes.append(self.pick_sizes( dims, num_cuts, cut_size_mode, cut_sizes_should_use_large_dimension ))\n",
        "        if num_cut_maps > 0:\n",
        "            sizes.append(self.pick_sizes( dims, num_cut_maps, 'cut_map' ))\n",
        "        \n",
        "        sizes = torch.cat( sizes )\n",
        "        hsizes = (sizes/2).int()\n",
        "\n",
        "        over = ( sizes * self.oversample )\n",
        "\n",
        "        center = torch.lerp( ( hsizes - over ).float(), ( dims - 1 - ( hsizes - over ) ).float(), torch.rand(self.cutn,2) ).int()\n",
        "        center = torch.where( hsizes > ( dims - 1 - hsizes ), hdims,  # center image in cut if bigger than image\n",
        "                     center.clip(hsizes, dims - 1 - hsizes))          # or clamp cut box to be within bounds\n",
        "\n",
        "        offsets = ( center - hsizes )\n",
        "        extents = ( offsets + sizes )\n",
        "        \n",
        "        # calculate cut area relative to whole\n",
        "        weights = ( extents - offsets ).clamp_max(dims).prod(dim=1)\n",
        "\n",
        "        return sizes, offsets, extents, weights.to(device)\n",
        "\n",
        "    def make_cuts(self,input,cut_size,offsets,extents):\n",
        "\n",
        "        # pad image to be square        \n",
        "        dims = torch.tensor(input.shape[2:4])\n",
        "        padding = ((dims.max() - dims )/2).int()\n",
        "\n",
        "        mean = input.view(3,-1).mean(dim=1)\n",
        "\n",
        "        fill = TF.pad( torch.zeros_like(input), ( padding[1], padding[0], padding[1], padding[0] ), padding_mode='constant', fill=1)\n",
        "        fill = fill * mean.view(1,3,1,1)\n",
        "\n",
        "        input = TF.pad( input, ( padding[1], padding[0], padding[1], padding[0] ), padding_mode='constant')\n",
        "\n",
        "        offsets += padding\n",
        "        extents += padding\n",
        "\n",
        "        fill_rand_mul = torch.FloatTensor(self.cutn).uniform_(0,2.0)\n",
        "\n",
        "        cutouts = []\n",
        "        for i in range( self.cutn ):\n",
        "            cutout = input[...,offsets[i,0]:extents[i,0],offsets[i,1]:extents[i,1]] \\\n",
        "                + fill[...,offsets[i,0]:extents[i,0],offsets[i,1]:extents[i,1]] * fill_rand_mul[i]\n",
        "            cutout = resample(cutout, (cut_size, cut_size))\n",
        "            cutouts.append(cutout)\n",
        " \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        return cutouts\n",
        "\n",
        "    def forward(self,input):\n",
        "        sizes, offsets, extents, weights = self.pick_cuts(input)        \n",
        "        cutouts = self.make_cuts(input,self.min_cut,offsets,extents)\n",
        "        \n",
        "        return cutouts, weights\n",
        "\n",
        "def show_images(images, width=20, height=5, columns = 5):\n",
        "    height = max(height, int(len(images)/columns) * height)\n",
        "    plt.figure(figsize=(width,height))\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "should_test = False #@param {type:'boolean'}\n",
        "show_histogram = False #@param {type:'boolean'}\n",
        "show_cuts = False #@param {type:'boolean'}\n",
        "\n",
        "if should_test:\n",
        "    a = torch.rand( 1,3,496, 994 )\n",
        "    #a = TF.to_tensor(Image.open('test.png')).unsqueeze(0)\n",
        "\n",
        "    mk = MakeCutouts3( 224, 64, oversample=cut_oversample, std_factor =std_factor )\n",
        "\n",
        "    seed = random.randrange(1,99999) \n",
        "    torch.random.manual_seed( seed )\n",
        "    print('seed:', seed)\n",
        "\n",
        "    if show_histogram:\n",
        "        sizes,heat = mk.simulate(a)\n",
        "\n",
        "        n,bins,patches = plt.hist(sizes,16,density=False, alpha=0.75)\n",
        "        plt.xlabel('Cut Sizes',size=16)\n",
        "        plt.ylabel('Counts',size=16)\n",
        "        plt.title('Cut_Pow Method',size=18)\n",
        "        plt.show()\n",
        "\n",
        "        display.display(TF.to_pil_image(heat[0].cpu()))\n",
        "\n",
        "    if show_cuts:\n",
        "        cuts, weights = mk(a)\n",
        "\n",
        "        images = [ TF.to_pil_image(c[0].cpu()) for c in cuts[:64].split(1)]\n",
        "        show_images(images, columns=5)\n",
        "        \n",
        "          #display.display(TF.to_pil_image(c[0].cpu()))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6uJtAOiZNAwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDvGKLWVKqj1",
        "cellView": "form"
      },
      "source": [
        "#@title Loss Module Definitions\n",
        "from typing import cast, Dict, Optional\n",
        "from kornia.augmentation import IntensityAugmentationBase2D\n",
        "\n",
        "class FixPadding(nn.Module):\n",
        "    \n",
        "    def __init__(self, module=None, threshold=1e-12, noise_frac=0.00 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.threshold = threshold\n",
        "        self.noise_frac = noise_frac\n",
        "\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self,input):\n",
        "\n",
        "        dims = input.shape\n",
        "\n",
        "        if self.module is not None:\n",
        "            input = self.module(input + self.threshold)\n",
        "\n",
        "        light = input.new_empty(dims[0],1,1,1).uniform_(0.,2.)\n",
        "\n",
        "        mixed = input.view(*dims[:2],-1).sum(dim=1,keepdim=True)\n",
        "\n",
        "        black = mixed < self.threshold\n",
        "        black = black.view(-1,1,*dims[2:4]).type(torch.float)\n",
        "        black = kornia.filters.box_blur( black, (5,5) ).clip(0,0.1)/0.1\n",
        "\n",
        "        mean = input.view(*dims[:2],-1).sum(dim=2) / mixed.count_nonzero(dim=2)\n",
        "        mean = ( mean[:,:,None,None] * light ).clip(0,1)\n",
        "\n",
        "        fill = mean.expand(*dims)\n",
        "        if 0 < self.noise_frac:\n",
        "            rng = torch.get_rng_state()\n",
        "            fill = fill + torch.randn_like(mean) * self.noise_frac\n",
        "            torch.set_rng_state(rng)\n",
        "        \n",
        "        if self.module is not None:\n",
        "            input = input - self.threshold\n",
        "\n",
        "        return torch.lerp(input,fill,black)\n",
        "\n",
        "\n",
        "class MyRandomNoise(IntensityAugmentationBase2D):\n",
        "    def __init__(\n",
        "        self,\n",
        "        frac: float = 0.1,\n",
        "        return_transform: bool = False,\n",
        "        same_on_batch: bool = False,\n",
        "        p: float = 0.5,\n",
        "    ) -> None:\n",
        "        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0)\n",
        "        self.frac = frac\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.__class__.__name__ + f\"({super().__repr__()})\"\n",
        "\n",
        "    def generate_parameters(self, shape: torch.Size) -> Dict[str, torch.Tensor]:\n",
        "        noise = torch.FloatTensor(1).uniform_(0,self.frac)\n",
        "        \n",
        "        # generate pixel data without throwing off determinism of augs\n",
        "        rng = torch.get_rng_state()\n",
        "        noise = noise * torch.randn(shape)\n",
        "        torch.set_rng_state(rng)\n",
        "\n",
        "        return dict(noise=noise)\n",
        "\n",
        "    def apply_transform(\n",
        "        self, input: torch.Tensor, params: Dict[str, torch.Tensor], transform: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        return input + params['noise'].to(input.device)\n",
        "\n",
        "\n",
        "class MultiClipLoss(nn.Module):\n",
        "    perceptors = {}\n",
        "\n",
        "    def __init__(self, clip_models, normalize_prompt_weights, cutn, cut_weight_pow=0.5, clip_weight=1.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalize_prompt_weights = normalize_prompt_weights\n",
        "\n",
        "        # Load Clip\n",
        "        for cm in { cm['model'] for cm in clip_models }:\n",
        "            p = clip.load(cm, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "            self.perceptors[cm] = { 'clip':p, 'res': int(p.visual.input_resolution) }\n",
        "\n",
        "        self.perceptor_work = clip_models\n",
        "        for pw in self.perceptor_work:\n",
        "            p = self.perceptors[ pw['model']]\n",
        "            pw['embeds'] = self.get_prompt_embeds( p['clip'], pw['prompt'] )\n",
        "            pw['res'] = p['res']\n",
        "    \n",
        "        self.perceptor_work.sort(key=lambda e: e['res'], reverse=True)\n",
        "        \n",
        "        # Make Cutouts\n",
        "        for p in self.perceptors:\n",
        "            print( p )\n",
        "\n",
        "        self.cut_sizes = list( { p['res'] for p in self.perceptors.values() } )\n",
        "        self.cut_sizes.sort( reverse=True )\n",
        "        \n",
        "        self.cutter = MakeCutouts3(self.cut_sizes[-1], cutn, oversample=0.5, std_factor=1)\n",
        "        self.cut_weight_pow = cut_weight_pow\n",
        "\n",
        "        # Prep Augments\n",
        "        self.noise_fac = 0.1\n",
        "        self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])    \n",
        "            \n",
        "        self.noise_aug = MyRandomNoise(frac=self.noise_fac,p=1.)\n",
        "\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomSharpness(0.3,p=0.1),\n",
        "            FixPadding( nn.Sequential(\n",
        "                K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='zeros'), # padding_mode=2\n",
        "                K.RandomPerspective(0.2,p=0.4, ),\n",
        "            )),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "            K.RandomGrayscale(p=0.15), \n",
        "            self.noise_aug,\n",
        "        )\n",
        "\n",
        "        self.clip_weight = clip_weight\n",
        "        \n",
        "\n",
        "    def get_prompt_embeds( self, perceptor, text_prompt ):\n",
        "    \n",
        "            texts = [phrase.strip() for phrase in text_prompt.split(\"|\")]\n",
        "            if text_prompt == ['']:\n",
        "                texts = []\n",
        "\n",
        "            prompts_weight_sum = 0\n",
        "            parsed_prompts = []\n",
        "            for prompt in texts:\n",
        "                txt, weight, stop = parse_prompt(prompt)\n",
        "                parsed_prompts.append( [txt,weight,stop] )\n",
        "                prompts_weight_sum += max( weight, 0 )\n",
        "\n",
        "            prompt_embeds = []\n",
        "            for prompt in parsed_prompts:\n",
        "                txt, weight, stop = prompt\n",
        "                clip_token = clip.tokenize(txt).to(device)\n",
        "\n",
        "                if self.normalize_prompt_weights and 0 < prompts_weight_sum:\n",
        "                    weight /= prompts_weight_sum\n",
        "\n",
        "                embed = perceptor.encode_text(clip_token).float()\n",
        "                embed_normed = F.normalize(embed.unsqueeze(0), dim=2)\n",
        "                prompt_embeds.append({'embed_normed':embed_normed,'weight':torch.as_tensor(weight, device=device),'stop':torch.as_tensor(stop, device=device)})\n",
        "        \n",
        "            return prompt_embeds\n",
        "\n",
        "    def make_cuts(self,img,currentres,offsets,extents,i):  \n",
        "           \n",
        "        cuts = self.cutter.make_cuts(img,currentres,offsets,extents)\n",
        "        cuts = clamp_with_grad(cuts,0,1)\n",
        "        cuts = self.augs( cuts )\n",
        "        cuts = self.normalize(cuts)\n",
        "        \n",
        "        return cuts\n",
        "\n",
        "    def forward( self, i, img ):\n",
        "        \n",
        "        sizes, offsets, extents, weights = self.cutter.pick_cuts(img)\n",
        "\n",
        "        weights = weights[:,None] ** self.cut_weight_pow\n",
        "        weights_sum = weights.sum()\n",
        "        \n",
        "        loss = []\n",
        "        \n",
        "        cuts = None\n",
        "        currentres = 0\n",
        "        \n",
        "        rng = torch.get_rng_state()\n",
        "\n",
        "        for pw in self.perceptor_work:\n",
        "            if currentres != pw['res']:\n",
        "                currentres = pw['res']\n",
        "\n",
        "                torch.set_rng_state(rng) \n",
        "                cuts = self.make_cuts( img, currentres, offsets, extents, i )\n",
        "\n",
        "            p = self.perceptors[pw['model']]\n",
        "            iii = p['clip'].encode_image(cuts).float()\n",
        "            input_normed = F.normalize(iii.unsqueeze(1), dim=2)\n",
        "            for prompt in pw['embeds']:\n",
        "                dists = input_normed.sub(prompt['embed_normed']).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "                dists = dists * prompt['weight'].sign()\n",
        "\n",
        "                dists = replace_grad(dists, torch.maximum(dists, prompt['stop']))\n",
        "\n",
        "###############################################################################                \n",
        "                \n",
        "                #l = dists.mean()\n",
        "                l = ( dists * weights ).sum() / weights_sum\n",
        "                \n",
        "###############################################################################    \n",
        "                \n",
        "                loss.append(l * prompt['weight'].abs() * pw['weight'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "class MSEDecayLoss(nn.Module):\n",
        "    def __init__(self, init_weight, mse_decay_rate, mse_epoches, mse_quantize ):\n",
        "        super().__init__()\n",
        "      \n",
        "        self.init_weight = init_weight\n",
        "        self.has_init_image = False\n",
        "        self.mse_decay = init_weight / mse_epoches if init_weight else 0 \n",
        "        self.mse_decay_rate = mse_decay_rate\n",
        "        self.mse_weight = init_weight\n",
        "        self.mse_epoches = mse_epoches\n",
        "        self.mse_quantize = mse_quantize\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def set_target( self, z_tensor, model ):\n",
        "        z_tensor = z_tensor.detach().clone()\n",
        "        if self.mse_quantize:\n",
        "            z_tensor = vector_quantize(z_tensor.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)#z.average\n",
        "        self.z_orig = z_tensor\n",
        "          \n",
        "    def forward( self, i, z ):\n",
        "        if self.is_active(i):\n",
        "            return F.mse_loss(z, self.z_orig) * self.mse_weight / 2\n",
        "        return 0\n",
        "        \n",
        "    def is_active(self, i):\n",
        "        if not self.init_weight:\n",
        "          return False\n",
        "        if i <= self.mse_decay_rate and not self.has_init_image:\n",
        "          return False\n",
        "        return True\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step( self, i ):\n",
        "\n",
        "        if i % self.mse_decay_rate == 0 and i != 0 and i < self.mse_decay_rate * self.mse_epoches:\n",
        "            \n",
        "            if self.mse_weight - self.mse_decay > 0 and self.mse_weight - self.mse_decay >= self.mse_decay:\n",
        "              self.mse_weight -= self.mse_decay\n",
        "            else:\n",
        "              self.mse_weight = 0\n",
        "            print(f\"updated mse weight: {self.mse_weight}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "  \n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agEFaX64bjdj",
        "cellView": "form"
      },
      "source": [
        "#@title Random Inits\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def rand_perlin_2d(shape, res, fade = lambda t: 6*t**5 - 15*t**4 + 10*t**3):\n",
        "    delta = (res[0] / shape[0], res[1] / shape[1])\n",
        "    d = (shape[0] // res[0], shape[1] // res[1])\n",
        "    \n",
        "    grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(0, res[1], delta[1])), dim = -1) % 1\n",
        "    angles = 2*math.pi*torch.rand(res[0]+1, res[1]+1)\n",
        "    gradients = torch.stack((torch.cos(angles), torch.sin(angles)), dim = -1)\n",
        "    \n",
        "    tile_grads = lambda slice1, slice2: gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]].repeat_interleave(d[0], 0).repeat_interleave(d[1], 1)\n",
        "    dot = lambda grad, shift: (torch.stack((grid[:shape[0],:shape[1],0] + shift[0], grid[:shape[0],:shape[1], 1] + shift[1]  ), dim = -1) * grad[:shape[0], :shape[1]]).sum(dim = -1)\n",
        "    \n",
        "    n00 = dot(tile_grads([0, -1], [0, -1]), [0,  0])\n",
        "    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\n",
        "    n01 = dot(tile_grads([0, -1],[1, None]), [0, -1])\n",
        "    n11 = dot(tile_grads([1, None], [1, None]), [-1,-1])\n",
        "    t = fade(grid[:shape[0], :shape[1]])\n",
        "    return math.sqrt(2) * torch.lerp(torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1])\n",
        "\n",
        "def rand_perlin_2d_octaves( desired_shape, octaves=1, persistence=0.5):\n",
        "    shape = torch.tensor(desired_shape)\n",
        "    shape = 2 ** torch.ceil( torch.log2( shape ) )\n",
        "    shape = shape.type(torch.int)\n",
        "\n",
        "    max_octaves = int(min(octaves,math.log(shape[0])/math.log(2), math.log(shape[1])/math.log(2)))\n",
        "    res = torch.floor( shape / 2 ** max_octaves).type(torch.int)\n",
        "\n",
        "    noise = torch.zeros(list(shape))\n",
        "    frequency = 1\n",
        "    amplitude = 1\n",
        "    for _ in range(max_octaves):\n",
        "        noise += amplitude * rand_perlin_2d(shape, (frequency*res[0], frequency*res[1]))\n",
        "        frequency *= 2\n",
        "        amplitude *= persistence\n",
        "    \n",
        "    return noise[:desired_shape[0],:desired_shape[1]]\n",
        "\n",
        "def rand_perlin_rgb( desired_shape, amp=0.1, octaves=6 ):\n",
        "  r = rand_perlin_2d_octaves( desired_shape, octaves )\n",
        "  g = rand_perlin_2d_octaves( desired_shape, octaves )\n",
        "  b = rand_perlin_2d_octaves( desired_shape, octaves )\n",
        "  rgb = ( torch.stack((r,g,b)) * amp + 1 ) * 0.5\n",
        "  return rgb.unsqueeze(0).clip(0,1).to(device)\n",
        "\n",
        "\n",
        "def pyramid_noise_gen(shape, octaves=5, decay=1.):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    max_octaves = int(min(math.log(h)/math.log(2), math.log(w)/math.log(2)))\n",
        "    if octaves is not None and 0 < octaves:\n",
        "      max_octaves = min(octaves,max_octaves)\n",
        "    for i in reversed(range(max_octaves)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += ( torch.randn([n, c, h_cur, w_cur]) / max_octaves ) * decay**( max_octaves - (i+1) )\n",
        "    return noise\n",
        "\n",
        "def rand_z(model, toksX, toksY):\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "    z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "    return z\n",
        "\n",
        "\n",
        "def make_rand_init( mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f ):\n",
        "\n",
        "  if mode == 'VQGAN ZRand':\n",
        "    return rand_z(model, toksX, toksY)\n",
        "  elif mode == 'Perlin Noise':\n",
        "    rand_init = rand_perlin_rgb((toksY * f, toksX * f), perlin_weight, perlin_octaves )\n",
        "    z, *_ = model.encode(rand_init * 2 - 1)\n",
        "    return z\n",
        "  elif mode == 'Pyramid Noise':\n",
        "    rand_init = pyramid_noise_gen( (1,3,toksY * f, toksX * f), pyramid_octaves, pyramid_decay).to(device)\n",
        "    rand_init = ( rand_init * 0.5 + 0.5 ).clip(0,1)\n",
        "    z, *_ = model.encode(rand_init * 2 - 1)\n",
        "    return z\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJmiZ8mRks_"
      },
      "source": [
        "# Make some Art!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxcx0j6zmePl",
        "cellView": "form"
      },
      "source": [
        "import json\n",
        "from PIL import Image, ExifTags\n",
        "from PIL.PngImagePlugin import PngImageFile, PngInfo\n",
        "\n",
        "#@title üå©Ô∏è Set VQGAN Model Save Location\n",
        "#@markdown It's a lot faster to load model files from google drive than to download them every time you want to use this notebook.\n",
        "save_vqgan_models_to_drive = False #@param {type: 'boolean'}\n",
        "download_all = False \n",
        "test_all = False\n",
        "vqgan_path_on_google_drive = \"/content/drive/MyDrive/Art/Models/VQGAN/\" #@param {type: 'string'}\n",
        "vqgan_path_on_google_drive += \"/\" if not vqgan_path_on_google_drive.endswith('/') else \"\"\n",
        "\n",
        "#@markdown ### üíæ Save Runs to Google Drive\n",
        "#@markdown Should all the images during the run be saved to google drive?\n",
        "save_output_to_drive = False #@param {type:'boolean'}\n",
        "output_path_on_google_drive = \"/content/drive/MyDrive/Art/\" #@param {type: 'string'}\n",
        "output_path_on_google_drive += \"/\" if not output_path_on_google_drive.endswith('/') else \"\"\n",
        "\n",
        "#@markdown Should the run params be saved as well for future reference? \n",
        "save_params_to_json = False #@param {type:'boolean'}\n",
        "\n",
        "#@markdown When saving the images, how much should be included in the name?\n",
        "shortname_limit = 50 #@param {type: 'number'}\n",
        "filename_limit = 250\n",
        "\n",
        "if save_vqgan_models_to_drive or save_output_to_drive:\n",
        "    from google.colab import drive    \n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "vqgan_model_path = \"/content/\"\n",
        "if save_vqgan_models_to_drive:\n",
        "    vqgan_model_path = vqgan_path_on_google_drive\n",
        "    !mkdir -p \"$vqgan_path_on_google_drive\"\n",
        "\n",
        "save_output_path = \"/content/art/\"\n",
        "if save_output_to_drive:\n",
        "    save_output_path = output_path_on_google_drive\n",
        "!mkdir -p \"$save_output_path\"\n",
        "\n",
        "model_download={\n",
        "  \"vqgan_imagenet_f16_1024\":\n",
        "      [[\"vqgan_imagenet_f16_1024.yaml\", \"https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\"],\n",
        "      [\"vqgan_imagenet_f16_1024.ckpt\", \"http://batbot.tv/ai/models/VQGAN/imagenet_1024_slim.ckpt\"]],\n",
        "  \"vqgan_imagenet_f16_16384\": \n",
        "      [[\"vqgan_imagenet_f16_16384.yaml\", \"http://batbot.tv/ai/models/VQGAN/imagenet_16384.yaml\"],\n",
        "      [\"vqgan_imagenet_f16_16384.ckpt\", \"http://batbot.tv/ai/models/VQGAN/imagenet_16384_slim.ckpt\"]],\n",
        "  \"vqgan_openimages_f8_8192\":\n",
        "      [[\"vqgan_openimages_f8_8192.yaml\", \"https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\"],\n",
        "      [\"vqgan_openimages_f8_8192.ckpt\", \"http://batbot.tv/ai/models/VQGAN/gumbel_f8_8192.ckpt\"]],\n",
        "  \"coco\":\n",
        "      [[\"coco_first_stage.yaml\", \"http://batbot.tv/ai/models/VQGAN/coco_first_stage.yaml\"],\n",
        "      [\"coco_first_stage.ckpt\", \"http://batbot.tv/ai/models/VQGAN/coco_first_stage.ckpt\"]],\n",
        "  \"faceshq\":\n",
        "      [[\"faceshq.yaml\", \"http://batbot.tv/ai/models/VQGAN/faceshq.yaml\"],\n",
        "      [\"faceshq.ckpt\", \"http://batbot.tv/ai/models/VQGAN/faceshq2_slim.ckpt\"]],\n",
        "  \"wikiart_1024\":\n",
        "      [[\"wikiart_1024.yaml\", \"http://batbot.tv/ai/models/VQGAN/WikiArt_augmented_Steps_7mil_finetuned_1mil.yaml\"],\n",
        "      [\"wikiart_1024.ckpt\", \"http://batbot.tv/ai/models/VQGAN/WikiArt_augmented_Steps_7mil_finetuned_1mil.ckpt\"]],\n",
        "  \"wikiart_16384\":\n",
        "      [[\"wikiart_16384.yaml\", \"http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml\"],\n",
        "      [\"wikiart_16384.ckpt\", \"http://batbot.tv/ai/models/VQGAN/wikiart_16384_slim.ckpt\"]],\n",
        "  \"sflckr\":\n",
        "      [[\"sflckr.yaml\", \"https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1\"],\n",
        "      [\"sflckr.ckpt\", \"http://batbot.tv/ai/models/VQGAN/sflckr_slim.ckpt\"]],\n",
        "  }\n",
        "\n",
        "loaded_model = None\n",
        "loaded_model_name = None\n",
        "def dl_vqgan_model(image_model):\n",
        "    for curl_opt in model_download[image_model]:\n",
        "        modelpath = f'{vqgan_model_path}{curl_opt[0]}'\n",
        "        if not path.exists(modelpath):\n",
        "            print(f'downloading {modelpath} from {curl_opt[1]}')\n",
        "            !curl -L -o {modelpath} '{curl_opt[1]}'\n",
        "        else:\n",
        "            print(f'found existing {curl_opt[0]}')\n",
        "\n",
        "def get_vqgan_model(image_model):\n",
        "    global loaded_model\n",
        "    global loaded_model_name\n",
        "    if loaded_model is None or loaded_model_name != image_model:\n",
        "        dl_vqgan_model(image_model)\n",
        "    \n",
        "        print(f'loading {image_model} vqgan checkpoint')\n",
        "\n",
        "        \n",
        "        vqgan_config= vqgan_model_path + model_download[image_model][0][0]\n",
        "        vqgan_checkpoint= vqgan_model_path + model_download[image_model][1][0]\n",
        "        print('vqgan_config',vqgan_config)\n",
        "        print('vqgan_checkpoint',vqgan_checkpoint)\n",
        "\n",
        "        model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)\n",
        "        if image_model == 'vqgan_openimages_f8_8192':\n",
        "            model.quantize.e_dim = 256\n",
        "            model.quantize.n_e = model.quantize.n_embed\n",
        "            model.quantize.embedding = model.quantize.embed\n",
        "\n",
        "        loaded_model = model\n",
        "        loaded_model_name = image_model\n",
        "\n",
        "    return loaded_model\n",
        "\n",
        "emoji_match = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "def slugify(value):\n",
        "    value = str(value)\n",
        "    value = re.sub(r':([-\\d.]+)', ' [\\\\1]', value)\n",
        "    value = re.sub(r'[|]','; ',value)\n",
        "    value = re.sub(r'[<>:\"/\\\\|?*]', ' ', value)\n",
        "    value = emoji_match.sub( r'', value )\n",
        "    return value.strip()\n",
        "\n",
        "save_output_path_for_run = \"\"\n",
        "def set_drive_output_subfolder( seconds=True ):\n",
        "    now = datetime.now()    \n",
        "    if seconds:\n",
        "        ts = now.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
        "    else:\n",
        "        ts = now.strftime(\"%Y-%m-%d %H-%M\")\n",
        "\n",
        "    run_args.timestamp = ts\n",
        "\n",
        "    global save_output_path_for_run\n",
        "    save_output_path_for_run = path.join( save_output_path, ts )\n",
        "\n",
        "has_saved_params_json = False\n",
        "def BuildRunRecord():\n",
        "    set_drive_output_subfolder()\n",
        "\n",
        "    global has_saved_params_json\n",
        "    has_saved_params_json = False\n",
        "\n",
        "\n",
        "def OutputRunRecordIfNeeded():\n",
        "    global has_saved_params_json\n",
        "    if save_params_to_json and not has_saved_params_json:\n",
        "        has_saved_params_json = True\n",
        "\n",
        "        fname = f'run_{run_args.gen_seed}.json'\n",
        "        fpath = path.join(save_output_path_for_run,fname)\n",
        "        with open(fpath, \"w\") as outfile:\n",
        "            json.dump( vars(run_args), outfile, indent=2)\n",
        "\n",
        "def get_filename(text, seed, i, ext):\n",
        "    text = re.split(r'[|:;]',text, 1)[0][:shortname_limit]\n",
        "    text = slugify(text)\n",
        "\n",
        "    ts =  run_args.timestamp\n",
        "    return f'{ts} r{seed}; {text}{ext}'\n",
        "\n",
        "last_image_saved = None\n",
        "def SaveRunIteration( i, pil ):\n",
        "    if not save_art_output:\n",
        "        return        \n",
        "        \n",
        "    if not path.exists(save_output_path_for_run):\n",
        "        os.makedirs(save_output_path_for_run)\n",
        "    \n",
        "    OutputRunRecordIfNeeded()\n",
        "\n",
        "    def makename(text, ts,  i):\n",
        "        text = re.split(r'[|:;]',text, 1)[0][:shortname_limit]\n",
        "        text = slugify(text)\n",
        "        return f'{ts} i{i}; {text}.png'\n",
        "\n",
        "    run_args.i = i\n",
        "\n",
        "    name = makename( run_args.text_prompt, run_args.timestamp, i )\n",
        "\n",
        "    global last_image_saved\n",
        "    last_image_saved = path.join( save_output_path_for_run, name ) \n",
        "\n",
        "    if save_params_to_exif:\n",
        "        jsondata = json.dumps( vars(run_args) ) \n",
        "\n",
        "        info = PngInfo()\n",
        "        info.add_text(\"RunArgs\", jsondata)\n",
        "\n",
        "        pil.save(last_image_saved, pnginfo=info)\n",
        "\n",
        "    else:\n",
        "        pil.save(last_image_saved)\n",
        "\n",
        "for model in model_download.keys():\n",
        "    if test_all:\n",
        "        try:\n",
        "            vqtest = get_vqgan_model(model)\n",
        "            del vqtest\n",
        "        except:\n",
        "            print(\"Failed to load \"+model)\n",
        "    elif save_vqgan_models_to_drive and download_all:\n",
        "        dl_vqgan_model(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUMYhUaEbnkL",
        "cellView": "form"
      },
      "source": [
        "#@title üëÅÔ∏è‚Äçüó®Ô∏è Set Display Rate\n",
        "#@markdown If `use_automatic_display_schedule` is enabled, the image will be output frequently at first, and then more spread out as time goes on. Turn this off if you want to specify the display rate yourself.\n",
        "use_automatic_display_schedule = True #@param {type:'boolean'}\n",
        "should_display_init = True #@param {type:'boolean'}\n",
        "display_every = 50 #@param {type:'number'}\n",
        "\n",
        "def should_checkin(i):\n",
        "  if i == 0:\n",
        "    return should_display_init\n",
        "\n",
        "  if i == max_iter: \n",
        "    return True \n",
        "\n",
        "  if not use_automatic_display_schedule:\n",
        "    return i % display_every == 0\n",
        "\n",
        "  schedule = [[100,25],[500,50],[1000,100],[2000,200]]\n",
        "  for s in schedule:\n",
        "    if i <= s[0]:\n",
        "      return i % s[1] == 0\n",
        "  return i % 500 == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üë®‚Äçüíª Advanced Features\n",
        "These are some extra features that are disabled by default but can help to push your art even further!\n",
        "\n",
        "<font size=5>‚ùó</font> *You need to run these cells even if you are not using these features.*"
      ],
      "metadata": {
        "id": "IbrDcvJNNLO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### ‚úíÔ∏è Prompt Writing Tips\n",
        "#@markdown \n",
        "#@markdown There are a few special syntax features you can use in your prompts.\n",
        "#@markdown \n",
        "#@markdown 1. You can combine multiple individual prompts by seperating them with pipes.<br>&nbsp;&nbsp;&nbsp;`The Enchanted Garden by James Gurney | Path to the Sacred Library`\n",
        "#@markdown 1. You can add weighting to the prompts by including a colon and a number after each prompt. By default, positive weights are normalized to not unbalance the mse and tv weights.  <br>&nbsp;&nbsp;&nbsp;`The Enchanted Garden by James Gurney:2 | Path to the Sacred Library:1`\n",
        "#@markdown 1. You can put multiple options in brackets seperated by a pipe so that one will be picked at random each type a new image is being generated.<br>&nbsp;&nbsp;&nbsp;```The {Enchanted|Forbidden} Garden by James Gurney```"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XTbjtvoNNNL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß† Madlib Prompts\n",
        "#@markdown When enabled, prompts will be generated at random using a templating language. This allows for very expressive prompts and combines well with batch runs but does require doing some code edits.\n",
        "#@markdown - Check out my [demo notebook](https://colab.research.google.com/drive/1gGwD0zDvyx0OnJU4KLjE8HrwnTlzI2rS?usp=sharing) for more explanation and examples\n",
        "AllowSimpleMadlibSyntax = True #on by default, to support {|} format in prompts\n",
        "UseFullMadlibTemplates = False #@param {type:'boolean'}\n",
        "\n",
        "## Template definition setup\n",
        "\n",
        "ml_templates = [ \n",
        "  { \n",
        "      \"prompt\" : \"A {something}, {something} garden by {James Gurney|Greg Rutkowski}\", \n",
        "      \"lookup\" : {}, \n",
        "      \"weight\":2 # increase the frequency of this prompt\n",
        "  },\n",
        "  {   \n",
        "      \"prompt\" : \"A beautiful {color} {sunrise|sunset} over the {something} {something} garden by {James Gurney|Greg Rutkowski}\", \n",
        "      \"lookup\" : {\n",
        "          \"something\": [\"enchanted\"], # overriding the base set\n",
        "          \"color\" : ['blue','red','purple','pink'] # adding extra data for only this prompt\n",
        "      } \n",
        "  } \n",
        "] \n",
        "ml_lookup = {\n",
        "    \"something\":['vivid','enchanted','forbidden','rotten']\n",
        "}\n",
        "ml_options = {\n",
        "    \"avoid_repeats\" : True, \n",
        "    \"fix_indefinite_articles\" : True \n",
        "} \n",
        "\n",
        "## -------------------------------- \n",
        "## supporting code from linked notebook\n",
        "\n",
        "import re\n",
        "import random\n",
        "import copy\n",
        "\n",
        "def madlib_prompt(base, lookup = {},\n",
        "        options = { \n",
        "            \"avoid_repeats\" : True, \n",
        "            \"fix_indefinite_articles\" : True \n",
        "        }):\n",
        "    lookup_cardstack = {}\n",
        "\n",
        "    def on_madlib_sub( match ):\n",
        "        opt = match.group(2).split('|')\n",
        "\n",
        "        key = opt[0]\n",
        "\n",
        "        # do a lookup if there's only one option in the brackets\n",
        "        if len(opt) == 1 and lookup.get(key):\n",
        "\n",
        "            # discard used words to avoid repeats, unless no words remain\n",
        "            if options[\"avoid_repeats\"]:\n",
        "\n",
        "                if len(lookup_cardstack.get(key) or []) == 0:\n",
        "                    lookup_cardstack[key] = copy.copy( lookup[key] )\n",
        "\n",
        "                g2 = random.choice( lookup_cardstack[key] )\n",
        "                lookup_cardstack[key].remove(g2)\n",
        "\n",
        "               \n",
        "            else:\n",
        "                g2 = random.choice( lookup.get(key) )\n",
        "\n",
        "        # choose one of the options to fill this space\n",
        "        else:\n",
        "            g2 = random.choice( opt )\n",
        "\n",
        "        # if the previous word is 'A' or 'An', figure out if the 'n' is needed\n",
        "        g1 = match.group(1)\n",
        "        if g1 is not None:\n",
        "            if options[\"fix_indefinite_articles\"]:\n",
        "                if g2[0].lower() in list(\"aeiou\"):\n",
        "                    g1 = g1[0] + 'n'\n",
        "                else:\n",
        "                    g1 = g1[0]\n",
        "            g2 = g1 + ' ' + g2\n",
        "        \n",
        "        return g2\n",
        "    \n",
        "    # find madlib spots, and replace\n",
        "    return re.sub(r\"(\\b[Aa]n? )?{([^{}]*)}\", on_madlib_sub, base )\n",
        "\n",
        "def madlib_template( templates, global_lookup = {}, \n",
        "        options = { \n",
        "            \"avoid_repeats\" : True, \n",
        "            \"fix_indefinite_articles\" : True \n",
        "        }):\n",
        "\n",
        "    # get prompt template weights and pick one\n",
        "    weights = [ p.get('weight') or 1 for p in templates]\n",
        "    t = random.choices( templates, weights=weights, k=1 )[0]\n",
        "    \n",
        "    lookup = { **global_lookup, **(t.get('lookup') or {}) }\n",
        "    lookup_cardstack = {}\n",
        "\n",
        "    def on_madlib_sub( match ):\n",
        "        opt = match.group(2).split('|')\n",
        "\n",
        "        key = opt[0]\n",
        "\n",
        "        # do a lookup if there's only one option in the brackets\n",
        "        if len(opt) == 1 and lookup.get(key):\n",
        "\n",
        "            # discard used words to avoid repeats, unless no words remain\n",
        "            if options[\"avoid_repeats\"]:\n",
        "\n",
        "                if len(lookup_cardstack.get(key) or []) == 0:\n",
        "                    lookup_cardstack[key] = copy.copy( lookup[key] )\n",
        "\n",
        "                g2 = random.choice( lookup_cardstack[key] )\n",
        "                lookup_cardstack[key].remove(g2)\n",
        "\n",
        "            else:\n",
        "                g2 = random.choice( lookup.get(key) )\n",
        "          \n",
        "        # or just pick one of the given options  \n",
        "        else:\n",
        "            g2 = random.choice( opt )\n",
        "\n",
        "        g1 = match.group(1)\n",
        "        if g1 is not None:\n",
        "\n",
        "            # if the previous word is 'A' or 'An', figure out if the 'n' is needed\n",
        "            if options[\"fix_indefinite_articles\"]:\n",
        "                    if g2[0].lower() in list(\"aeiou\"):\n",
        "                        g1 = g1[0] + 'n'\n",
        "                    else:\n",
        "                        g1 = g1[0]\n",
        "            g2 = g1 + ' ' + g2\n",
        "            \n",
        "        return g2\n",
        "    \n",
        "    # find madlib spots, and replace\n",
        "    return re.sub(r\"(\\b[Aa]n? )?{([^{}]*)}\", on_madlib_sub, t['prompt'] )\n",
        "\n",
        "## -------------------------------- \n",
        "## hook into the generation\n",
        "def get_prompt():\n",
        "\n",
        "    if UseFullMadlibTemplates:\n",
        "        # generate prompt from template        \n",
        "        if 'base_prompt' in run_args:\n",
        "            del run_args.base_prompt\n",
        "        return madlib_template( ml_templates, ml_lookup, ml_options )\n",
        "        \n",
        "    # should still use simple madlibbing functionality \n",
        "    # when not using an advanced template\n",
        "    if AllowSimpleMadlibSyntax:\n",
        "        return madlib_prompt(run_args.base_prompt)\n",
        "\n",
        "    return run_args.base_prompt\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7EiEqI1rNPJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîÅ Batch Runs\n",
        "#@markdown When enabled, this notebook will continue to make art. Use at your own risk! This can cause your colab account to be locked out temporarily if you abuse it too much.\n",
        "#@markdown - When doing batch runs, the gen seed will be randomized for each piece.\n",
        "#@markdown - If `batch_run_limit` is -1, it will keep going until you run out of disk space or Google disconnects you!\n",
        "\n",
        "BatchRunOverride = False #@param {type:'boolean'}\n",
        "batch_run_limit = 5 #@param {type:'number'}\n",
        "headless = False #@param {type:'boolean'}\n",
        "\n",
        "#@markdown Note: If you edit the code in this cell you can add some parameter randomization here as well.\n",
        "\n",
        "def rand_float( start, stop ):\n",
        "    return torch.FloatTensor(1).uniform_(start,stop).item()\n",
        "\n",
        "def randomize_run():        \n",
        "    global run_args\n",
        "\n",
        "    # Setup Random Seed\n",
        "    if BatchRunOverride:\n",
        "        run_args.gen_seed = update_random( -1, 'randomized run') \n",
        "    else:\n",
        "        run_args.gen_seed = update_random( run_args.gen_seed, 'randomized run' )\n",
        "   \n",
        "    prompt = get_prompt()\n",
        "    prompt = prompt.encode('utf-16', 'surrogatepass').decode('utf-16') #fix emoji issue...\n",
        "\n",
        "    run_args.text_prompt = prompt\n",
        "\n",
        "    # run_args.clip1_weight = random.choice( [ 0.33, 0.5, 0.66 ])\n",
        "    # run_args.tv_weight = 0\n",
        "\n",
        "    # run_args.rand_init_mode = random.choice( [ \"Perlin Noise\", \"Pyramid Noise\"] )\n",
        "\n",
        "    # run_args.perlin_octaves = random.choice( [random.randint(2,3), random.randint(6,8)] )\n",
        "    # run_args.perlin_weight = rand_float(0.1,0.4)\n",
        "    \n",
        "    # run_args.pyramid_octaves = random.randint(7,8)\n",
        "    # run_args.pyramid_decay = rand_float(0.7,0.9)\n",
        "\n",
        "    # run_args.mse_weight = rand_float( 0.1,0.2 )\n",
        "\n",
        "    # run_args.learning_rate = random.choice( [0.4,0.5,0.2] )\n",
        "    # run_args.learning_rate_epoch = random.choice( [0.4,0.2,0.3] )\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AXDXArQrNR9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## üí• Noise Burst\n",
        "#@markdown Increase the amount of noise before sending the image to CLIP. This creates a large burst of noise with an exponential falloff every so often (based on the parameters below).\n",
        "\n",
        "DoNoiseBurst = False #@param {type:'boolean'}\n",
        "noise_frac_base = 0.1 #@param {type:'number'}\n",
        "#@markdown  ---\n",
        "noise_frac_burst = 0.3 #@param {type:'number'}\n",
        "noise_burst_start = 150 #@param {type:'number'}\n",
        "noise_burst_end = 350 #@param {type:'number'}\n",
        "noise_burst_freq = 100 #@param {type:'number'}\n",
        "noise_burst_length = 100 #@param {type:'number'}\n",
        "\n",
        "def get_noise_frac(i):\n",
        "    if not DoNoiseBurst:\n",
        "        return run_args.noise_frac_base\n",
        "        \n",
        "    nb_start = run_args.noise_burst_start\n",
        "    nb_end = run_args.noise_burst_end\n",
        "\n",
        "    noise = run_args.noise_frac_base\n",
        "\n",
        "    if nb_start <= i and i <= nb_end:\n",
        "        nb_freq = run_args.noise_burst_freq\n",
        "        nb_length = run_args.noise_burst_length\n",
        "        alpha = math.exp( -10 * ( (i + nb_start) % nb_freq ) / nb_length )\n",
        "        noise +=  run_args.noise_frac_burst * alpha\n",
        "\n",
        "    return noise\n",
        "    "
      ],
      "metadata": {
        "cellView": "form",
        "id": "LCJIEiJpNTmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # ‚úÇÔ∏è Cut Distribution Tuning\n",
        "\n",
        "#@markdown Cut sizes are picked by one of two methods:\n",
        "#@markdown 1. A truncated normal about an average point between the perceptor size and the smaller dimension of the image\n",
        "#@markdown 2. A weighted choice from a map of 4 possible sizes:<br>  ```[Perceptor size, Min image dimension, Max image dimension, Geometric average of min and max dimension]```\n",
        "#@markdown\n",
        "#@markdown `cut_map_weight` determines how many of the cuts are allocated via truncated normal vs weighted map. The default tuning is set to match the distribution of jbusted1's setup, with a small portion of cuts also being allocated for medium-large and global to help with overall image cohesion.\n",
        "\n",
        "cut_map_weight = 0.66#@param {type:'number'}\n",
        "cut_map_size_weights = [0.6, 0.4, 0.2, 0.12] #@param\n",
        "\n",
        "std_factor = 1.5#@param {type:'number'}\n",
        "#@markdown - Used in truncated normal to pick random cut sizes\n",
        "\n",
        "cut_oversample = 0#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown - Increase to add oversampling of the edges to get more even cut distribution\n",
        "\n",
        "#@markdown ###üíª Simulate\n",
        "#@markdown You can use the options below to render out a heatmap and histogram to get a better idea of how the MakeCuts method is picking cut sizes.\n",
        "cut_test_show_histogram = False #@param {type:'boolean'}\n",
        "cut_test_show_cuts = False\n",
        "cut_test_count = 64 #@param {type:'number'}\n",
        "\n",
        "try: cut_test_width = width\n",
        "except NameError: cut_test_width = 994\n",
        "\n",
        "try: cut_test_height = height\n",
        "except NameError: cut_test_height = 496\n",
        "\n",
        "cut_test_perceptor_dim = 224 #@param {type:'number'}\n",
        "\n",
        "def test_makecuts():\n",
        "    if cut_test_show_histogram or cut_test_show_cuts:\n",
        "        \n",
        "        mk = MakeCutouts3( cut_test_perceptor_dim, cut_test_count, oversample=cut_oversample, std_factor =std_factor )\n",
        "    \n",
        "        if cut_test_show_histogram:\n",
        "            \n",
        "            a = torch.rand( 1,3,cut_test_height, cut_test_width )\n",
        "            #a = TF.to_tensor(Image.open('test.png')).unsqueeze(0)\n",
        "\n",
        "            seed = random.randrange(1,99999) \n",
        "            torch.random.manual_seed( seed )\n",
        "            print('seed:', seed)\n",
        "\n",
        "            sizes,heat = mk.simulate(a)\n",
        "\n",
        "            n,bins,patches = plt.hist(sizes,16,density=False, alpha=0.75)\n",
        "            plt.xlabel('Cut Sizes',size=16)\n",
        "            plt.ylabel('Counts',size=16)\n",
        "            plt.title('Cut_Pow Method',size=18)\n",
        "            plt.show()\n",
        "\n",
        "            display.display(TF.to_pil_image(heat[0].cpu()))\n",
        "\n",
        "        if cut_test_show_cuts:\n",
        "            cuts, weights = mk(a)\n",
        "\n",
        "            images = [ TF.to_pil_image(c[0].cpu()) for c in cuts[:cut_test_count].split(1)]\n",
        "            show_images(images, columns=5)\n",
        "        \n",
        "test_makecuts()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zX9CYaFONVD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###üé¨ Video\n",
        "#@markdown If you want to generate a video of the run, you need to save the frames as you go. The more frequently you save, the longer the video but the slower it will take to generate.\n",
        "save_frames_for_video = True #@param {type:'boolean'}\n",
        "autogenerate_video_after_run = True #@param {type:'boolean'}\n",
        "\n",
        "video_start =  0#@param {type:'number'}\n",
        "video_step =   3#@param {type:'number'}\n",
        "\n",
        "#@markdown Setting a higher frequency will make a longer video, and a higher framerate will make a shorter video.\n",
        "fps = 24 #@param{type:'number'}\n",
        "\n",
        "def should_save_for_video(i):\n",
        "    if not save_frames_for_video or i < video_start:\n",
        "        return False\n",
        "    \n",
        "    return (i - video_start) % video_step == 0\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_frame_for_video(i, pil):\n",
        "    frame = (i - video_start) // video_step\n",
        "    pil.save(f'steps/step{frame:04}.png')   \n",
        "\n",
        "@torch.no_grad()\n",
        "def save_video():\n",
        "    if not save_frames_for_video:\n",
        "        print('You must first enable save_frames_for_video in the Advanced section to use this feature.')\n",
        "        return\n",
        "\n",
        "    !mkdir -p \"/content/video/\"\n",
        "    vname = \"/content/video/\"+get_filename(run_args.text_prompt,run_args.gen_seed,None,'.mp4')\n",
        "          \n",
        "    !ffmpeg -y -v 1 -framerate $fps -i steps/step%04d.png -r $fps -vcodec libx264 -crf 32 -pix_fmt yuv420p \"$vname\"\n",
        "    if save_output_to_drive:\n",
        "        if not path.exists(save_output_path_for_run):\n",
        "            os.makedirs(save_output_path_for_run)\n",
        "        OutputRunRecordIfNeeded()\n",
        "        !cp \"$vname\" \"$save_output_path_for_run\"\n",
        "\n",
        "    mp4 = open(vname,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display.display( display.HTML(f'<video controls><source src=\"{data_url}\" type=\"video/mp4\"></video>') )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZceqGjQSNWXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### ‚ú® Animation\n",
        "#@markdown Add optional zoom and rotate as you generate, to make cool animated videos. `save_frames_for_video` must be enabled above as well for this to really do anything useful.\n",
        "\n",
        "UseAnimationOverride = False #@param {type:'boolean'}\n",
        "\n",
        "#@markdown You should sync these to the video start/step times for best results.\n",
        "anim_start = 100  #@param {type:'number'}\n",
        "anim_step = 3 #@param {type:'number'}\n",
        "\n",
        "#@markdown Animation parameters for basic animation. Modify the code in this cell to create more complex behaviors.\n",
        "zoom_amount = 4  #@param {type:'number'}\n",
        "rotate_amount = -0.5  #@param {type:'number'}\n",
        "\n",
        "def should_do_animation_step(i):\n",
        "    if not UseAnimationOverride or i < anim_start:\n",
        "        return False\n",
        "    return (i - anim_start) % anim_step == 0\n",
        "\n",
        "@torch.no_grad()\n",
        "def do_anim_step():\n",
        "    global z\n",
        "    zoom = zoom_amount\n",
        "\n",
        "    # get current frame\n",
        "    if use_ema_tensor:\n",
        "        out = synth( z.average )\n",
        "    else:\n",
        "        out = synth( z.tensor )\n",
        "    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "    # do the transformations\n",
        "    sx, sy = pil.size\n",
        "    im1 = pil.resize((sx + zoom, sy + zoom), Image.LANCZOS)\n",
        "    im1 = im1.rotate(rotate_amount)\n",
        "    pil.paste(im1, (-zoom//2, -zoom//2))\n",
        "\n",
        "    #re-encode\n",
        "    z, *_ = model.encode(TF.to_tensor(pil).to(device).unsqueeze(0) * 2 - 1)\n",
        "    z = EMATensor(z, ema_val)\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hIXJZPzBNXry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìë Miscellaneous\n",
        "#@markdown These are some extra parameters you can tweak to change how some of the underlying systems operate during the generation. Only change if you know what you are doing!\n",
        "\n",
        "resume_from_last = False #@param {type:'boolean'}\n",
        "#@markdown - Use the last image generated as the init image for the next run\n",
        "\n",
        "save_params_to_exif = False #@param {type:'boolean'}\n",
        "#@markdown - Save run parameters into the metadata of the png file\n",
        "\n",
        "#multi_clip_should_match_cuts = True #@param {type:'boolean'}\n",
        "\n",
        "cut_weight_pow = 0.5 #@param {type:'number'}\n",
        "#@markdown - The power the length * width of the cut is raised to to calculate its prompt loss weight. 1.0 would make the weight be based on area. 0.5 is based on a square leg. 0 would disable.\n",
        "\n",
        "normalize_prompt_weights = True #@param {type:'boolean'}\n",
        "#@markdown - Should the total weight of the text prompts stay in the same range, relative to other loss functions?\n",
        "\n",
        "use_ema_tensor = False #@param {type:'boolean'}\n",
        "#@markdown - Enabling the EMA tensor will cause the image to be slower to generate but may help it be more cohesive.\n",
        "#@markdown This can also help keep the final image closer to the init image, if you are providing one.\n",
        "\n",
        "### MSE Loss Behavior\n",
        "mse_epoch_step = 50 #@param {type:'number'}\n",
        "mse_epoch_count = 5 #@param {type:'number'}\n",
        "\n",
        "\n",
        "\n",
        "advanced_cells_have_been_run = True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gmDLxJc4NY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbqnA_STd9bp"
      },
      "source": [
        "Before generating, the rest of the setup steps must first be executed by pressing **`Runtime > Run All`**. This only needs to be done once."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do the Run\n",
        "Before generating, all of the steps above must first be executed by pressing **`Runtime > Run All`**. This only needs to be done once.\n"
      ],
      "metadata": {
        "id": "vWdDv7SWNbOk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWGR8q9AT8uk",
        "cellView": "form"
      },
      "source": [
        "#@title ### üîÆ Inspiration\n",
        "#@markdown  What do you want to see?\n",
        "#@markdown  \n",
        "try: advanced_cells_have_been_run \n",
        "except NameError: advanced_cells_have_been_run = False\n",
        "if not advanced_cells_have_been_run:\n",
        "    print( \"ERROR: Before you can generate art, you need to run all the cells above this cell first -- including the cells in the Advanced Features section.\" )\n",
        "    raise StopExecution\n",
        "\n",
        "\n",
        "text_prompt = 'human language tree, photography'#@param {type:'string'}\n",
        "gen_seed = 69#@param {type:'number'}\n",
        "save_art_output = True #@param {type:'boolean'}\n",
        "\n",
        "#@markdown ### üß© Initialization\n",
        "#@markdown You can use an init image to achieve some more control. The init image can be a url or you can upload it to colab and just include a filename. Try using images from previous runs with new prompts to better direct and morph your art!\n",
        "init_image = 'https://i.imgur.com/9GbMvcv.png'#@param {type:'string'}\n",
        "width = 800#@param {type:'number'}\n",
        "height = 451#@param {type:'number'}\n",
        "max_iter = 150 #@param {type:'number'}\n",
        "\n",
        "#@markdown There are different ways of generating the random starting point, when not using an init image. These influence how the image turns out. The default VQGAN ZRand is good, but some models and subjects may do better with perlin or pyramid noise.\n",
        "#@markdown - If you want to keep starting from the same point, set `gen_seed` to a positive number. `-1` will make it random every time. \n",
        "rand_init_mode = 'VQGAN ZRand'#@param [ \"VQGAN ZRand\", \"Perlin Noise\", \"Pyramid Noise\"]\n",
        "perlin_octaves = 7#@param {type:\"slider\", min:1, max:8, step:1}\n",
        "perlin_weight = 0.22#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "pyramid_octaves = 5#@param {type:\"slider\", min:1, max:8, step:1}\n",
        "pyramid_decay = 0.99#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "ema_val = 0.99\n",
        "\n",
        "#@markdown ### üé® Generation\n",
        "#@markdown Picking a different VQGAN model will impact how an image generates. Think of this as giving the generator a different set of brushes and paints to work with. CLIP is still the \"eyes\" and is judging the image against your prompt but using different brushes will make a different image.\n",
        "#@markdown - `vqgan_imagenet_f16_16384` is the default and what most people use\n",
        "vqgan_model = 'vqgan_imagenet_f16_16384'#@param [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "\n",
        "#@markdown ###üëÄ Perception\n",
        "#@markdown How many slices of the image should be sent to CLIP each iteration to score? Higher numbers are better, but cost more memory. If you are running into memory issues try lowering this value.\n",
        "cut_n =  52#@param {type:'number'}\n",
        "\n",
        "#@markdown One clip model is good. Two is better? You may need to reduce the number of cuts to support having more than one CLIP model. CLIP is what scores the image against your prompt and each model has slightly different ideas of what things are.\n",
        "#@markdown - `ViT-B/32` is fast and good and what most people use to begin with\n",
        "\n",
        "clip_model = 'ViT-B/32' #@param [\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\",\"RN50\",\"RN101\",\"RN50x64\"]\n",
        "clip_model2 ='None' #@param [\"None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\", \"RN50\",\"RN101\",\"RN50x64\"]\n",
        "if clip_model2 == \"None\":\n",
        "    clip_model2 = None \n",
        "clip1_weight = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "#@markdown ### üìâ Learning Rates & Weights\n",
        "#@markdown Learning rates greatly impact how quickly an image can generate, or if an image can generate at all. The first learning rate is only for the first 50 iterations. The epoch rate is what is used after reaching the first mse epoch. \n",
        "#@markdown You can try lowering the epoch rate while raising the initial learning rate and see what happens\n",
        "learning_rate = 0.2#@param {type:'number'}\n",
        "learning_rate_epoch = 0.2#@param {type:'number'}\n",
        "#@markdown How much should we try to match the init image, or if no init image how much should we resist change after reaching the first epoch?\n",
        "mse_weight = 0.5 #@param {type:'number'}\n",
        "#@markdown Adding some TV may make the image blurrier but also helps to get rid of noise. A good value to try might be 0.1.\n",
        "tv_weight = 0.0 #@param {type:'number'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown  <font size=\"5\">üòä</font> I'd love to see what you can make with my notebook. Tweet me your art [@remi_durant](https://twitter.com/remi_durant)!\n",
        "\n",
        "output_as_png = True\n",
        "\n",
        "run_args = argparse.Namespace(\n",
        "    base_prompt = text_prompt,\n",
        "    text_prompt = text_prompt,\n",
        "    clip_model = clip_model,\n",
        "    clip_model2 = clip_model2,\n",
        "    clip1_weight = clip1_weight,\n",
        "    gen_seed = gen_seed,\n",
        "    #clip_models = clip_models,\n",
        "    width = width,\n",
        "    height = height,\n",
        "    rand_init_mode = rand_init_mode,\n",
        "    perlin_octaves = perlin_octaves,\n",
        "    perlin_weight = perlin_weight,\n",
        "    pyramid_octaves = pyramid_octaves,\n",
        "    pyramid_decay = pyramid_decay,\n",
        "    cut_n = cut_n,\n",
        "    cut_weight_pow = cut_weight_pow,\n",
        "    learning_rate = learning_rate,\n",
        "    learning_rate_epoch = learning_rate_epoch,\n",
        "    mse_weight = mse_weight,\n",
        "    mse_epoch_step = mse_epoch_step,\n",
        "    mse_epoch_count = mse_epoch_count,\n",
        "    tv_weight = tv_weight,\n",
        "    normalize_prompt_weights = normalize_prompt_weights,\n",
        "    vqgan_model = vqgan_model,\n",
        "\n",
        "    noise_frac_base = noise_frac_base,\n",
        ")\n",
        "\n",
        "if DoNoiseBurst:\n",
        "    run_args.noise_burst_start = noise_burst_start\n",
        "    run_args.noise_burst_end = noise_burst_end\n",
        "    run_args.noise_burst_freq = noise_burst_freq\n",
        "    run_args.noise_burst_length = noise_burst_length\n",
        "    run_args.noise_frac_burst = noise_frac_burst\n",
        "\n",
        "print('Using device:', device)\n",
        "print('using prompts: ', text_prompt)\n",
        "\n",
        "MultiClipLoss.perceptors = {} # remove any preloaded clip models\n",
        "clear_memory()\n",
        "\n",
        "model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "losses = []\n",
        "mb = master_bar(range(1))\n",
        "gnames = ['losses']\n",
        "\n",
        "mb.names=gnames\n",
        "mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "mb.graph_ax = axs\n",
        "mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "## init step\n",
        "def init():    \n",
        "    randomize_run()\n",
        "\n",
        "    BuildRunRecord()\n",
        "\n",
        "    if run_args.clip_model2:     \n",
        "        clip_models = [\n",
        "            {'model':run_args.clip_model, 'weight':run_args.clip1_weight, 'prompt':run_args.text_prompt},\n",
        "            {'model':run_args.clip_model2, 'weight':1.0 - run_args.clip1_weight, 'prompt':run_args.text_prompt},\n",
        "        ]\n",
        "    else:\n",
        "        clip_models = [\n",
        "            {'model':run_args.clip_model, 'weight':1.0, 'prompt':run_args.text_prompt},\n",
        "        ]\n",
        "\n",
        "    print( clip_models )\n",
        "\n",
        "    global clip_loss\n",
        "    clip_loss = MultiClipLoss( clip_models, \n",
        "        normalize_prompt_weights = run_args.normalize_prompt_weights, \n",
        "        cutn = run_args.cut_n, \n",
        "        cut_weight_pow = run_args.cut_weight_pow)\n",
        "\n",
        "    # reset seed before making init image\n",
        "    # (this should already be set to something other than -1 from calling randomize_run)\n",
        "    run_args.gen_seed = update_random( run_args.gen_seed, 'image generation')\n",
        "        \n",
        "    # Make Z Init\n",
        "    global z\n",
        "    z = 0\n",
        "\n",
        "    f = 2**(model.decoder.num_resolutions - 1)\n",
        "    toksX, toksY = math.ceil( run_args.width / f), math.ceil( run_args.height / f)\n",
        "\n",
        "    print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "    run_args.width = toksX * f\n",
        "    run_args.height = toksY * f\n",
        "    \n",
        "    global init_image\n",
        "    \n",
        "    if resume_from_last and last_image_saved is not None:\n",
        "        print(\"Resuming from last generation\")\n",
        "        init_image = last_image_saved\n",
        "\n",
        "    has_init_image = (init_image != \"\")\n",
        "    if has_init_image:\n",
        "        if 'http' in init_image:\n",
        "            req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            img = Image.open(urlopen(req))\n",
        "        else:\n",
        "            img = Image.open(init_image)\n",
        "\n",
        "        pil_image = img.convert('RGB')\n",
        "        pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "        pil_image = TF.to_tensor(pil_image)\n",
        "        #if args.use_noise:\n",
        "        #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "        z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "        del pil_image\n",
        "        del img\n",
        "\n",
        "    else:\n",
        "        z = make_rand_init( run_args.rand_init_mode, model, run_args.perlin_octaves, run_args.perlin_weight, run_args.pyramid_octaves, run_args.pyramid_decay, toksX, toksY, f )\n",
        "        \n",
        "    z = EMATensor(z, ema_val)\n",
        "    \n",
        "    global opt\n",
        "    opt = optim.Adam( z.parameters(), lr=run_args.learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "    global mse_loss\n",
        "    mse_loss = MSEDecayLoss( run_args.mse_weight, \n",
        "                            mse_decay_rate = mse_epoch_step, \n",
        "                            mse_epoches = mse_epoch_count, \n",
        "                            mse_quantize=True )\n",
        "    mse_loss.set_target( z.tensor, model )\n",
        "    mse_loss.has_init_image = has_init_image\n",
        "\n",
        "    global tv_loss\n",
        "    tv_loss = TVLoss() \n",
        "\n",
        "## optimizer loop\n",
        "\n",
        "def synth(z, quantize=True, scramble=True):\n",
        "    z_q = 0\n",
        "    if quantize:\n",
        "      z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "    else:\n",
        "      z_q = z.model\n",
        "\n",
        "    out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, z, out_pil, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "\n",
        "    print( 'Prompt:', run_args.text_prompt )\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "\n",
        "    display_format='png' if output_as_png else 'jpg'\n",
        "    pil_data = image_to_data_url(out_pil, display_format)\n",
        "    \n",
        "    display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "def train(i):\n",
        "    global opt\n",
        "    global z \n",
        "    opt.zero_grad( set_to_none = True )\n",
        "\n",
        "    out = checkpoint( synth, z.tensor )\n",
        "\n",
        "    clip_loss.noise_aug.frac = get_noise_frac(i)\n",
        "\n",
        "    lossAll = []\n",
        "    lossAll += clip_loss( i,out )\n",
        "\n",
        "    if 0 < run_args.mse_weight:\n",
        "        msel = mse_loss(i,z.tensor)\n",
        "        if 0 < msel:\n",
        "            lossAll.append(msel)\n",
        "    \n",
        "    if 0 < run_args.tv_weight:\n",
        "        lossAll.append(tv_loss(out)*run_args.tv_weight)\n",
        "    \n",
        "    loss = sum(lossAll)\n",
        "    loss.backward()\n",
        "\n",
        "    if should_checkin(i) or should_save_for_video(i):\n",
        "        with torch.no_grad():\n",
        "            if use_ema_tensor:\n",
        "                out = synth( z.average )\n",
        "\n",
        "            pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "            if should_checkin(i):\n",
        "                if not headless:\n",
        "                    checkin(i, z, pil, lossAll)\n",
        "                SaveRunIteration( i, pil )\n",
        "            \n",
        "            if should_save_for_video(i):\n",
        "                save_frame_for_video(i, pil)   \n",
        "\n",
        "    # update graph\n",
        "    losses.append(loss.item())\n",
        "    x = range(len(losses))\n",
        "    mb.update_graph( [[x,losses]] )\n",
        "\n",
        "    opt.step()\n",
        "    if use_ema_tensor:\n",
        "      z.update()\n",
        "\n",
        "set_drive_output_subfolder()\n",
        "\n",
        "run_limit = 1\n",
        "if BatchRunOverride:\n",
        "    run_limit = batch_run_limit\n",
        "\n",
        "from IPython.display import clear_output \n",
        "runs = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while runs < run_limit or run_limit == -1:\n",
        "            init()\n",
        "\n",
        "            i = 0\n",
        "            losses = []\n",
        "\n",
        "            # clear out folder for video frames\n",
        "            if save_frames_for_video:                                \n",
        "                !rm -r steps\n",
        "                !mkdir -p steps\n",
        "            \n",
        "            while True and i <= max_iter:\n",
        "        \n",
        "                if i % 200 == 0:\n",
        "                    clear_memory()\n",
        "\n",
        "                train(i)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    should_do_mse_epoch = mse_loss.step(i)\n",
        "                    if should_do_mse_epoch or should_do_animation_step(i):\n",
        "                        \n",
        "                        if should_do_mse_epoch:\n",
        "                            print(f'Reseting optimizer at mse epoch (i: {i})')\n",
        "\n",
        "                            if mse_loss.has_init_image and use_ema_tensor:\n",
        "                                mse_loss.set_target(z.average,model)\n",
        "                            else:\n",
        "                                mse_loss.set_target(z.tensor,model)                            \n",
        "                             \n",
        "                            # Make sure not to spike loss when mse_loss turns on\n",
        "                            if not mse_loss.is_active(i):\n",
        "                                z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                                z.tensor.requires_grad = True\n",
        "                            \n",
        "                        if should_do_animation_step(i):\n",
        "                            print(f'Applying animation step and reseting optimizer (i: {i})')\n",
        "                            do_anim_step()\n",
        "\n",
        "                            mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                        if use_ema_tensor:\n",
        "                            z = EMATensor(z.average, ema_val)\n",
        "                        else:\n",
        "                            z = EMATensor(z.tensor, ema_val)\n",
        "                        \n",
        "                        opt = optim.Adam(z.parameters(), lr=run_args.learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "                # step completed\n",
        "                if not BatchRunOverride:\n",
        "                    pbar.update()\n",
        "                i += 1\n",
        "                \n",
        "            if save_frames_for_video and autogenerate_video_after_run:\n",
        "                save_video()\n",
        "\n",
        "            # image completed\n",
        "            if BatchRunOverride:\n",
        "                pbar.update()\n",
        "            runs += 1\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaw63lWBqsnj",
        "cellView": "form"
      },
      "source": [
        "#@title üé• Make a Video of Your Last Run!\n",
        "#@markdown If you want to make a video, you must first enable `save_frames_for_video` in the Advanced section.\n",
        "\n",
        "save_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZnqxw4uM_7h"
      },
      "source": [
        "# Extra Resources\n",
        "\n",
        "You may want to check out some of my other projects as well to get more insight into how the different parts of VQGAN+CLIP work together to generate an image:\n",
        "\n",
        "1. [Artist Studies](https://remidurant.com/artists) - Find good artists for your prompts\n",
        "2. [How CLIP \"sees\"](https://twitter.com/remi_durant/status/1460607677801897990?s=20)\n",
        "3. Art Styles and Movements \n",
        " - [VQGAN Imagenet16k + ViT-B/32](https://imgur.com/gallery/BZzXLHY)\n",
        " - [VQGAN Imagenet16k + ViT-B/16](https://imgur.com/gallery/w14XZFd)\n",
        " - [VQGAN Imagenet16k + RN50x16](https://imgur.com/gallery/Kd0WYfo)\n",
        " - [VQGAN Imagenet16k + RN50x4](https://imgur.com/gallery/PNd7zYp)\n",
        "\n",
        "\n",
        "There is also this great prompt exploration from @kingdomakrillic which showcases a lot of the words you can add to your prompt to push CLIP towards certain styles:\n",
        "- [CLIP + VQGAN Keyword Comparison](https://imgur.com/a/SnSIQRu)\n"
      ]
    }
  ]
}