{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "CLIP+VQGAN adapted to video by *Thomash*.\n",
        "\n",
        "Based on a notebook by [Katherine Crowson](https://twitter.com/RiversHaveWings).\n",
        "\n",
        "Modified by [jbuster](https://twitter.com/jbusted1). and [thomash](https://twitter.com/pollinations_ai). \n",
        "\n",
        "---\n",
        "\n",
        "It is possible to provide multiple text prompts with weights by providing them like this: \n",
        "**```oil painting: 0.5|salvador dali: 0.3|edward much:0.6|robot friend: 1.0|text:-0.5```** *In this case a negative weight next to `text` makes the model avoid text.*  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZmCmyKM9fmv",
        "tags": [
          "parameters"
        ],
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# Text Prompt\n",
        "text_input = 'eyes by vladimir kush'  #@param {type: \"string\"}\n",
        "\n",
        "# Video prompt. \n",
        "video_file = 'blenderfire.mp4' #@param {type: \"string\"}\n",
        "\n",
        "# FPS to extract video at\n",
        "fps =  8#@param {type: \"number\"}\n",
        "\n",
        "# Width in pixels of image to be generated. If you put this too high the GPU may run out of memory. Rather use super-resolution to achieve high resolutions.\n",
        "width = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Height in pixels of image to be generated. (see width)\n",
        "height = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Apply a (neural) super-resolution step (2 x image width x image height)\n",
        "super_resolution = False #@param {type: \"boolean\"}\n",
        "\n",
        "# Step size (how fast to try and optimize the image. Between 0 and 100)\n",
        "step_size =  60 #@param {type: \"number\"}\n",
        "\n",
        "# Iterations (how many frames to optimize the image. Determines the length of the output video)\n",
        "iterations =  15 #@param {type: \"number\"}\n",
        "\n",
        "# Random seed. Each number will give a unique outpiut\n",
        "random_seed = 666 #@param {type: \"number\"}\n",
        "\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "social = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4LgXcz0UKvt"
      },
      "outputs": [],
      "source": [
        "!rm -rfv /content/frames\n",
        "!mkdir -p /content/frames\n",
        "!ffmpeg -i \"{video_file}\" -r {fps} /content/frames/frame%4d.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CUtyJQ0Ppv3"
      },
      "outputs": [],
      "source": [
        "# Model to use. ruDALLE was recently finetuned by a russian company called Sber and could give better outputs.\n",
        "imagemodel = \"imagenet\" \n",
        "#####@param ['imagenet', 'ruDALLE']\n",
        "\n",
        "\n",
        "# check if christmas and add christmas to prompt\n",
        "from datetime import datetime\n",
        "\n",
        "d = datetime.now()\n",
        "\n",
        "if social and (d.strftime(\"%m\") == \"12\") and  (d.strftime(\"%d\") == \"24\") or  (d.strftime(\"%d\") == \"25\") or  (d.strftime(\"%d\") == \"26\"):\n",
        "    text_input = text_input + \"|christmas:3|text:-0.5\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VIf9Z1CTsC7R"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "#@title Upscale images/video frames\n",
        "\n",
        "black_and_white = False\n",
        "\n",
        "loaded_upscale_model = False\n",
        "\n",
        "def upscale(filepath):\n",
        "  if not super_resolution:\n",
        "    return\n",
        "  global loaded_upscale_model\n",
        "  if not loaded_upscale_model:\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd /content/Real-ESRGAN\n",
        "    !git checkout 3338b31f486586bd7f6b20cc2a9fadd5ed192a00\n",
        "    # Set up the environment\n",
        "    !pip install git+https://github.com/xinntao/BasicSR.git\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    %cd -\n",
        "    loaded_upscale_model = True \n",
        "  \n",
        "  %cd /content/Real-ESRGAN\n",
        "  !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth --input $filepath --netscale 4 --outscale 2 --half --output $output_path\n",
        "  filepath_out = filepath.replace(\".jpg\",\"_out.jpg\")\n",
        "  !mv -v $filepath_out $filepath\n",
        "  %cd -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf54A275YLpt"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSfISAhyPmyp"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $output_path\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "#%cd taming-transformers\n",
        "#!git checkout 2908a53b88478e5812d619b6ac003dbb29b069a0\n",
        "#%cd -\n",
        "!pip install torch==1.8.1 torchtext\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhhdWrSxQhwg"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "!sudo apt install -f aria2\n",
        "\n",
        "model_mapping = {\n",
        "    \"ruDALLE\": { \n",
        "        \"name\": \"vqgan_openimages_f16_8192.ckpt\",\n",
        "        \"config\": \"https://pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt.yaml\",\n",
        "        \"checkpoint\": \"https://pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt\",\n",
        "    },\n",
        "    \"imagenet\": {\n",
        "        \"name\": \"vqgan_imagenet_f16_16384.ckpt\",\n",
        "        \"config\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n",
        "        \"checkpoint\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1',        \n",
        "    } \n",
        "}\n",
        "\n",
        "selected_vqgan = model_mapping[imagemodel]\n",
        "vqgan_name = selected_vqgan[\"name\"]\n",
        "\n",
        "config = selected_vqgan[\"config\"]\n",
        "checkpoint = selected_vqgan[\"checkpoint\"]\n",
        "\n",
        "if not Path(vqgan_name).exists():\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{config}' -o {vqgan_name}.yaml\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{checkpoint}'  -o {vqgan_name}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "import tensorflow\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from taming.models import cond_transformer, vqgan\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import kornia.augmentation as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvnTBhPGT1gn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLw9p5Rzacso"
      },
      "outputs": [],
      "source": [
        "step_size = step_size / 100\n",
        "step_size = step_size*step_size\n",
        "if vqgan_name == 'vqgan_openimages_f16_8192.ckpt':\n",
        "  step_size=step_size / 20\n",
        "\n",
        "z_orig = None\n",
        "mse_weight = None\n",
        "vid_index = 0\n",
        "previous_image = None\n",
        "previous_z = None\n",
        "def process_frame(image_prompt):\n",
        "  # scale step size\n",
        "  global z_orig, mse_weight\n",
        "    \n",
        "  args = argparse.Namespace(\n",
        "      \n",
        "      prompts=[t.strip() for t in text_input.split(\"|\")],\n",
        "      size=[width, height], \n",
        "      init_image=image_prompt,\n",
        "      init_weight= 0.5,\n",
        "\n",
        "      previous_image=previous_image,\n",
        "\n",
        "      # clip model settings\n",
        "      clip_model='ViT-B/32',\n",
        "      vqgan_config=f'{vqgan_name}.yaml',         \n",
        "      vqgan_checkpoint=vqgan_name,\n",
        "      step_size=step_size,\n",
        "      \n",
        "      # cutouts / crops\n",
        "      cutn=32,\n",
        "      cut_pow=1,\n",
        "      cut_size=224,\n",
        "\n",
        "      # display\n",
        "      display_freq=5,\n",
        "      seed=random_seed,\n",
        "      use_augs = True,\n",
        "      noise_fac= 0.1,\n",
        "\n",
        "      record_generation=True,\n",
        "\n",
        "      # noise and other constraints\n",
        "      use_noise = None,\n",
        "      constraint_regions = False,#\n",
        "      \n",
        "      \n",
        "      # add noise to embedding\n",
        "      noise_prompt_weights = None,\n",
        "      noise_prompt_seeds = [14575],#\n",
        "\n",
        "      # mse settings\n",
        "      mse_withzeros = True,\n",
        "      mse_decay_rate = 50,\n",
        "      mse_epoches = 10,\n",
        "\n",
        "      # end itteration\n",
        "      max_itter = iterations,\n",
        "  )\n",
        "\n",
        "  def noise_gen(shape):\n",
        "      n, c, h, w = shape\n",
        "      noise = torch.zeros([n, c, 1, 1])\n",
        "      for i in reversed(range(5)):\n",
        "          h_cur, w_cur = h // 2**i, w // 2**i\n",
        "          noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "          noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "      return noise\n",
        "\n",
        "\n",
        "  def sinc(x):\n",
        "      return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "  def lanczos(x, a):\n",
        "      cond = torch.logical_and(-a < x, x < a)\n",
        "      out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "      return out / out.sum()\n",
        "\n",
        "\n",
        "  def ramp(ratio, width):\n",
        "      n = math.ceil(width / ratio + 1)\n",
        "      out = torch.empty([n])\n",
        "      cur = 0\n",
        "      for i in range(out.shape[0]):\n",
        "          out[i] = cur\n",
        "          cur += ratio\n",
        "      return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "  def resample(input, size, align_corners=True):\n",
        "      n, c, h, w = input.shape\n",
        "      dh, dw = size\n",
        "\n",
        "      input = input.view([n * c, 1, h, w])\n",
        "\n",
        "      if dh < h:\n",
        "          kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "          pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "          input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "          input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "      if dw < w:\n",
        "          kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "          pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "          input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "          input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "      input = input.view([n, c, h, w])\n",
        "      return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "      \n",
        "\n",
        "  # def replace_grad(fake, real):\n",
        "  #     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "  class ReplaceGrad(torch.autograd.Function):\n",
        "      @staticmethod\n",
        "      def forward(ctx, x_forward, x_backward):\n",
        "          ctx.shape = x_backward.shape\n",
        "          return x_forward\n",
        "\n",
        "      @staticmethod\n",
        "      def backward(ctx, grad_in):\n",
        "          return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "  class ClampWithGrad(torch.autograd.Function):\n",
        "      @staticmethod\n",
        "      def forward(ctx, input, min, max):\n",
        "          ctx.min = min\n",
        "          ctx.max = max\n",
        "          ctx.save_for_backward(input)\n",
        "          return input.clamp(min, max)\n",
        "\n",
        "      @staticmethod\n",
        "      def backward(ctx, grad_in):\n",
        "          input, = ctx.saved_tensors\n",
        "          return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "  replace_grad = ReplaceGrad.apply\n",
        "\n",
        "  clamp_with_grad = ClampWithGrad.apply\n",
        "  # clamp_with_grad = torch.clamp\n",
        "\n",
        "  def vector_quantize(x, codebook):\n",
        "      d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "      indices = d.argmin(-1)\n",
        "      x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "      return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "  class Prompt(nn.Module):\n",
        "      def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "          super().__init__()\n",
        "          self.register_buffer('embed', embed)\n",
        "          self.register_buffer('weight', torch.as_tensor(weight))\n",
        "          self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "      def forward(self, input):\n",
        "          \n",
        "          input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "          embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\n",
        "\n",
        "          dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "          dists = dists * self.weight.sign()\n",
        "          return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "  def parse_prompt(prompt):\n",
        "      vals = prompt.rsplit(':', 2)\n",
        "      vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "      return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "  def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "      input_normed = F.normalize(input, dim=-1)\n",
        "      target_normed = F.normalize(target, dim=-1)\n",
        "      logits = input_normed @ target_normed.T * logit_scale\n",
        "      if labels is None:\n",
        "          labels = torch.arange(len(input), device=logits.device)\n",
        "      return F.cross_entropy(logits, labels)\n",
        "\n",
        "  class MakeCutouts(nn.Module):\n",
        "      def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "          super().__init__()\n",
        "          self.cut_size = cut_size\n",
        "          self.cutn = cutn\n",
        "          self.cut_pow = cut_pow\n",
        "\n",
        "          self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "          self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "      def set_cut_pow(self, cut_pow):\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "      def forward(self, input):\n",
        "          sideY, sideX = input.shape[2:4]\n",
        "          max_size = min(sideX, sideY)\n",
        "          min_size = min(sideX, sideY, self.cut_size)\n",
        "          cutouts = []\n",
        "          cutouts_full = []\n",
        "          \n",
        "          min_size_width = min(sideX, sideY)\n",
        "          lower_bound = float(self.cut_size/min_size_width)\n",
        "          \n",
        "          for ii in range(self.cutn):\n",
        "              \n",
        "              \n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "          \n",
        "          cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "          if args.use_augs:\n",
        "            cutouts = augs(cutouts)\n",
        "\n",
        "          if args.noise_fac:\n",
        "            facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "            cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "          \n",
        "\n",
        "          return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "  def load_vqgan_model(config_path, checkpoint_path):\n",
        "      config = OmegaConf.load(config_path)\n",
        "      if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "          model = vqgan.VQModel(**config.model.params)\n",
        "          model.eval().requires_grad_(False)\n",
        "          model.init_from_ckpt(checkpoint_path)\n",
        "      elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "          parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "          parent_model.eval().requires_grad_(False)\n",
        "          parent_model.init_from_ckpt(checkpoint_path)\n",
        "          model = parent_model.first_stage_model\n",
        "      elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "          model = vqgan.GumbelVQ(**config.model.params)\n",
        "          model.eval().requires_grad_(False)\n",
        "          model.init_from_ckpt(checkpoint_path)\n",
        "      else:\n",
        "          raise ValueError(f'unknown model type: {config.model.target}')\n",
        "      del model.loss\n",
        "      return model\n",
        "\n",
        "  def resize_image(image, out_size):\n",
        "      ratio = image.size[0] / image.size[1]\n",
        "      area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "      size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "      return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "  class TVLoss(nn.Module):\n",
        "      def forward(self, input):\n",
        "          input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "          x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "          y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "          diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "          return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "  class GaussianBlur2d(nn.Module):\n",
        "      def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "          super().__init__()\n",
        "          self.mode = mode\n",
        "          self.value = value\n",
        "          if not window:\n",
        "              window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "          if sigma:\n",
        "              kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "              kernel /= kernel.sum()\n",
        "          else:\n",
        "              kernel = torch.ones([1])\n",
        "          self.register_buffer('kernel', kernel)\n",
        "\n",
        "      def forward(self, input):\n",
        "          n, c, h, w = input.shape\n",
        "          input = input.view([n * c, 1, h, w])\n",
        "          start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "          end_pad = self.kernel.shape[0] // 2\n",
        "          input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "          input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "          input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "          return input.view([n, c, h, w])\n",
        "\n",
        "  class EMATensor(nn.Module):\n",
        "      \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "      def __init__(self, tensor, decay):\n",
        "          super().__init__()\n",
        "          self.tensor = nn.Parameter(tensor)\n",
        "          self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "          self.register_buffer('average', torch.zeros_like(tensor))\n",
        "          self.decay = decay\n",
        "          self.register_buffer('accum', torch.tensor(1.))\n",
        "          self.update()\n",
        "      \n",
        "      @torch.no_grad()\n",
        "      def update(self):\n",
        "          if not self.training:\n",
        "              raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "          self.accum *= self.decay\n",
        "          self.biased.mul_(self.decay)\n",
        "          self.biased.add_((1 - self.decay) * self.tensor)\n",
        "          self.average.copy_(self.biased)\n",
        "          self.average.div_(1 - self.accum)\n",
        "\n",
        "      def forward(self):\n",
        "          if self.training:\n",
        "              return self.tensor\n",
        "          return self.average\n",
        "\n",
        "  %mkdir /content/vids\n",
        "\n",
        "  mse_decay = 0\n",
        "  if args.init_weight:\n",
        "    mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "  # <AUGMENTATIONS>\n",
        "  augs = nn.Sequential(\n",
        "      \n",
        "      K.RandomHorizontalFlip(p=0.5),\n",
        "      K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "      K.RandomPerspective(0.2,p=0.4, ),\n",
        "      K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "      )\n",
        "\n",
        "  noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "  image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "  image.save('init3.png')\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  print('Using device:', device)\n",
        "  print('using prompts: ', args.prompts)\n",
        "\n",
        "  tv_loss = TVLoss() \n",
        "\n",
        "  model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "  perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "  mse_weight = args.init_weight\n",
        "\n",
        "  cut_size = args.cut_size\n",
        "  # e_dim = model.quantize.e_dim\n",
        "\n",
        "  if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "      e_dim = 256\n",
        "      n_toks = model.quantize.n_embed\n",
        "      z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "      z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "  else:\n",
        "      e_dim = model.quantize.e_dim\n",
        "      n_toks = model.quantize.n_e\n",
        "      z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "      z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "  make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "  f = 2**(model.decoder.num_resolutions - 1)\n",
        "  toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "\n",
        "  if args.seed is not None:\n",
        "      torch.manual_seed(args.seed)\n",
        "\n",
        "  if args.init_image:\n",
        "      pil_image = Image.open(args.init_image).convert('RGB')\n",
        "      pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "      pil_image = TF.to_tensor(pil_image)\n",
        "      if args.use_noise:\n",
        "        pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "      z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "  if args.previous_image:\n",
        "      previous_pil_image = Image.open(args.previous_image).convert('RGB')\n",
        "      previous_pil_image = previous_pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "      previous_pil_image = TF.to_tensor(previous_pil_image)\n",
        "      previous_z, *_ = model.encode(previous_pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "\n",
        "  else:\n",
        "      \n",
        "      one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "          z = one_hot @ model.quantize.embed.weight\n",
        "      else:\n",
        "          z = one_hot @ model.quantize.embedding.weight\n",
        "      z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "  if args.mse_withzeros and not args.init_image:\n",
        "    z_orig = torch.zeros_like(z)\n",
        "  else:\n",
        "    z_orig = z.clone()\n",
        "\n",
        "  z.requires_grad = True\n",
        "\n",
        "  opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  pMs = []\n",
        "\n",
        "  if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "      gen = torch.Generator().manual_seed(seed)\n",
        "      embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "      pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "  for prompt in args.prompts:\n",
        "      txt, weight, stop = parse_prompt(prompt)\n",
        "      embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "  def synth_gumbel(z, quantize=True, saturate=True):\n",
        "      logits = model.quantize.proj(z)\n",
        "      if quantize:\n",
        "          one_hot = F.gumbel_softmax(logits, tau=1, hard=True, dim=1)\n",
        "      else:\n",
        "          one_hot = F.one_hot(logits.argmax(1), logits.shape[1]).movedim(3, 1).to(logits.dtype)\n",
        "      z_q = torch.einsum('nchw,cd->ndhw', one_hot, model.quantize.embed.weight)\n",
        "      return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "  def synth(z, quantize=True, saturate=True):\n",
        "      out = None\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        out = synth_gumbel(z, quantize, saturate)\n",
        "      else:\n",
        "        if args.constraint_regions:\n",
        "          z = replace_grad(z, z * z_mask)\n",
        "\n",
        "        if quantize:\n",
        "          z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "        else:\n",
        "          z_q = z.model\n",
        "\n",
        "        out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "      if saturate and not image_prompt:\n",
        "        progress = i / args.max_itter\n",
        "        saturation = max(0,min(1,(progress - 0.25) * 2))\n",
        "        out = transforms.functional.adjust_saturation(out, saturation)\n",
        "      \n",
        "      return out\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def checkin(i, losses):\n",
        "      losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "      tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "      out = synth(z, True)# False)\n",
        "\n",
        "      TF.to_pil_image(out[0].cpu()).save(f'progress.png')   \n",
        "      #display.display(display.Image(args.init_image))\n",
        "      #display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "  def ascend_txt(i):\n",
        "      global mse_weight\n",
        "      global previous_image\n",
        "      out = synth(z)\n",
        "      if args.record_generation:\n",
        "        with torch.no_grad():\n",
        "          global vid_index\n",
        "          out_a = synth(z, True)#, False)\n",
        "          if i == args.max_itter -1:\n",
        "            filename = f'{output_path}/progress_{vid_index:05}.jpg'\n",
        "            TF.to_pil_image(out_a[0].cpu()).save(filename)\n",
        "            upscale(filename)\n",
        "            vid_index += 1\n",
        "            previous_image = filename\n",
        "\n",
        "      cutouts = make_cutouts(out)\n",
        "      cutouts = resample(cutouts, (perceptor.visual.input_resolution, perceptor.visual.input_resolution))\n",
        "\n",
        "\n",
        "      iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "      result = []\n",
        "\n",
        "      if args.init_weight:\n",
        "          \n",
        "          global z_orig, previous_z\n",
        "          \n",
        "          result.append(F.mse_loss(z, z_orig) * mse_weight / 2)\n",
        "          if previous_z:\n",
        "            result.append(F.mse_loss(z, previous_z) * mse_weight / 2)\n",
        "          # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "          with torch.no_grad():\n",
        "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "              if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "                mse_weight = mse_weight - mse_decay\n",
        "                print(f\"updated mse weight: {mse_weight}\")\n",
        "              else:\n",
        "                mse_weight = 0\n",
        "                print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "      for prompt in pMs:\n",
        "          result.append(prompt(iii))\n",
        "\n",
        "      return result\n",
        "\n",
        "  vid_index = 0\n",
        "  def train(i):\n",
        "      \n",
        "      opt.zero_grad()\n",
        "      lossAll = ascend_txt(i)\n",
        "\n",
        "      if i % args.display_freq == 0:\n",
        "          checkin(i, lossAll)\n",
        "      \n",
        "      loss = sum(lossAll)\n",
        "\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "  i = 0\n",
        "  try:\n",
        "      with tqdm() as pbar:\n",
        "          while True and i != args.max_itter:\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "                \n",
        "                opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "for image in sorted(glob(\"/content/frames/*.jpg\")):\n",
        "  print(\"processing video frame\", image)\n",
        "  process_frame(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# create video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT3hKb5gJUPq"
      },
      "outputs": [],
      "source": [
        "out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "!mkdir -p /tmp/ffmpeg\n",
        "!cp $output_path/*.jpg /tmp/ffmpeg\n",
        "last_frame=!ls -t /tmp/ffmpeg/*.jpg | head -1\n",
        "last_frame = last_frame[0]\n",
        "\n",
        "# Copy last frame to start and duplicate at end so it sticks around longer\n",
        "end_still_seconds = 4\n",
        "!cp -v $last_frame /tmp/ffmpeg/0000.jpg\n",
        "for i in range(end_still_seconds * 10):\n",
        "  pad_file = f\"/tmp/ffmpeg/zzzz_pad_{i:05}.jpg\"\n",
        "  !cp -v $last_frame $pad_file\n",
        "\n",
        "!ffmpeg  -r {fps} -i /tmp/ffmpeg/%*.jpg -y -c:v libx264 /tmp/vid_no_audio.mp4\n",
        "!ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n",
        "\n",
        "print(\"Written\", out_file)\n",
        "!sleep 2\n",
        "!rm -r /tmp/ffmpeg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text-To-Video - CLIP-Guided VQGAN (Video)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}