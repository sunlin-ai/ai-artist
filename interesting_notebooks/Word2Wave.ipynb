{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Wave.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJGVFXA3GuSQ"
      },
      "source": [
        "# Word2Wave: generate audio samples from a text prompt ðŸ“ --> ðŸŽ¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuJT1CT7HCS3"
      },
      "source": [
        "This notebook is a playground for [Word2Wave](https://github.com/ilaria-manco/word2wave), a simple method that uses [WaveGAN](https://arxiv.org/abs/1802.04208) to generate audio samples from text prompts by optimising audio-text similarity based on [COALA](https://arxiv.org/abs/2006.08386) embeddings.\n",
        "\n",
        "Author: [@Ilaria__Manco](https://twitter.com/ilaria__manco)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_S1fdcxSsHl"
      },
      "source": [
        "Before you start, make sure you select a (free) GPU to work with: `Runtime > Change runtime type > Hardware accelerator > GPU`. Then check that this step worked by executing the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st781Y_SE8vP"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1YfXYNLS2un"
      },
      "source": [
        "#@title Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8FeeeIx_BbQ"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJYe1vYsY_AG"
      },
      "source": [
        "#@markdown Install Word2Wave, import necessary packages\n",
        "\n",
        "!git clone https://github.com/ilaria-manco/word2wave\n",
        "%cd /content/word2wave/\n",
        "!pip3  install -r requirements.txt\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import logging\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "from torchaudio import transforms as T\n",
        "\n",
        "from IPython import display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import output, files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Download model from Pollinations IPFS\n",
        "!mkdir -p /content/models\n",
        "!wget -N https://pollinations.ai/ipfs/QmfRSNhj4z8bmVzrhcUTeSLd3PPBGrUJJB1w5tML1dtjB5/gan_fs_loop_3.tar -P /content/models"
      ],
      "metadata": {
        "id": "gRZsp3IUrCxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iGlN6SA-qe7"
      },
      "source": [
        "#@markdown Copy the pre-trained WaveGAN and COALA weights from drive\n",
        "drive_path = \"/content/model\" #@param {type:\"string\"}\n",
        "\n",
        "!cp -r -v {drive_path}\"wavegan\" \"/content/word2wave/wavegan\"\n",
        "\n",
        "# if the pre-trained coala models are in a gdrive folder, copy them here\n",
        "!cp -r -v {drive_path}\"coala\" \"/content/word2wave/coala/\"\n",
        "!mv \"/content/word2wave/coala/coala/\" \"/content/word2wave/coala/models/\"\n",
        "\n",
        "# othertwise, download them\n",
        "!wget https://raw.githubusercontent.com/xavierfav/coala/master/saved_models/dual_e_c/audio_encoder_epoch_200.pt\n",
        "!wget https://raw.githubusercontent.com/xavierfav/coala/master/saved_models/dual_e_c/tag_encoder_epoch_200.pt\n",
        "\n",
        "\n",
        "\n",
        "!mkdir \"/content/output/\"\n",
        "!mkdir \"/content/output/audio/\"\n",
        "!mkdir \"/content/output/latents/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKelPkcJ2Ad",
        "cellView": "form"
      },
      "source": [
        "#@title Define some helper functions\n",
        "\n",
        "def sample_noise(size, latent_dim):\n",
        "  noise = torch.FloatTensor(size, latent_dim)\n",
        "  noise.data.normal_()\n",
        "  return noise\n",
        "\n",
        "\n",
        "def latent_space_interpolation(model, n_samples=10, source=None, target=None):\n",
        "  if source is None and target is None:\n",
        "    random_samples = sample_noise(2, 100)\n",
        "    source = random_samples[0]\n",
        "    target = random_samples[1]\n",
        "  with torch.no_grad():\n",
        "    interpolated_z = []\n",
        "    for alpha in np.linspace(0, 1, n_samples):\n",
        "      interpolation = alpha * source + ((1 - alpha) * target)\n",
        "      interpolated_z.append(interpolation.cuda())\n",
        "\n",
        "    interpolated_z = torch.stack(interpolated_z)\n",
        "    generated_audio = model(interpolated_z)\n",
        "  return generated_audio\n",
        "\n",
        "\n",
        "def save_audio(audio_to_save):\n",
        "    librosa.output.write_wav(os.path.join(\"/content/audio/\", text + \".wav\"), audio_to_save, 16000)\n",
        "\n",
        "\n",
        "def check_text_input(text):\n",
        "  _, words_in_dict, words_not_in_dict = word2wave.tokenize_text(text)\n",
        "  if not words_in_dict:\n",
        "      raise Exception(\"All the words in the text prompt are out-of-vocabulary, please try with another prompt\")\n",
        "  elif words_not_in_dict:\n",
        "      missing_words = \", \".join(words_not_in_dict)\n",
        "      logging.info(\"Out-of-vocabulary words found, ignoring: \\\"{}\\\"\".format(missing_words))\n",
        "  logging.info(\"Making sounds to match the following text: {}\".format(\" \".join(words_in_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuw0PC0Qwyh-",
        "cellView": "form"
      },
      "source": [
        "#@title Settings\n",
        "model_name = 'gan_fs_loop_3' #@param [\"gan_drum\",\"gan_fs_loop_3\", \"gan_fs_loop_4\"] {type:\"string\"}\n",
        "audio_save_freq =  10 #@param {type:\"number\"}\n",
        "learning_rate =  0.02#@param {type:\"number\"}\n",
        "training_steps = 10000 #@param {type:\"number\"}\n",
        "threshold = 0.11 #@param {type:\"number\"}\n",
        "verbose = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3hsLObYVxqK"
      },
      "source": [
        "## Generate audio from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Re4fhQEVgzb",
        "cellView": "form"
      },
      "source": [
        "#@markdown\n",
        "\n",
        "available_tags = widgets.Dropdown(options=[tag for id, tag in id2tag.items()], value='laughing')\n",
        "available_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3pQbOevI5dC",
        "cellView": "form"
      },
      "source": [
        "#@title Input a text prompt\n",
        " \n",
        "#@markdown This can be an arbitrary combination of words from those available in the dropdown menu above (run the cell first to see the menu)\n",
        "text = \"firework\" #@param {type:\"string\"}\n",
        " \n",
        "import ipywidgets as widgets\n",
        "import json\n",
        " \n",
        "id2tag = json.load(open('/content/word2wave/coala/id2token_top_1000.json', 'rb'))\n",
        " \n",
        "check_text_input(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJxXOKHBXYXA",
        "cellView": "form"
      },
      "source": [
        "#@markdown Load pre-trained WaveGAN and generate an audio sample from a random latent vector drawn from a Gaussian distribution\n",
        " \n",
        "from word2wave import Word2Wave\n",
        " \n",
        "class Config():\n",
        "  def __init__(self):\n",
        "    self.coala_model_name = \"dual_e_c\"\n",
        "    self.wavegan_path = \"/content/word2wave/wavegan/{}.tar\".format(model_name)\n",
        " \n",
        "config = Config()\n",
        "word2wave = Word2Wave(config).cuda()\n",
        " \n",
        "original_audio, loss = word2wave(text)\n",
        " \n",
        "ipd.Audio(original_audio.detach().cpu().numpy(), rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k42dFb0czbeN",
        "cellView": "form"
      },
      "source": [
        "#@title Run training cycle\n",
        "\n",
        "for name, param in word2wave.named_parameters():\n",
        "  if name != \"latents\" and \"generator\" not in name:\n",
        "  # if name != \"latents\":\n",
        "      param.requires_grad = False \n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "params=[word2wave.latents],\n",
        "lr=learning_rate,\n",
        "betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "i = 0\n",
        "\n",
        "while i < training_steps:\n",
        "    audio, loss = word2wave(text)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if verbose and i % 100 == 0:\n",
        "        print(f'Step {i}', f'|| Loss: {loss.data.cpu().numpy()[0]}')\n",
        "\n",
        "    if loss < threshold:\n",
        "        break\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0nC1yPubJ3h",
        "cellView": "form"
      },
      "source": [
        "#@title Listen to generated audio { run: \"auto\" }\n",
        "\n",
        "save_output = True #@param {type:\"boolean\"}\n",
        "save_dir = '/content/output/' #@param {type:\"string\"}\n",
        "\n",
        "generated_audio = audio.squeeze().detach().cpu().numpy()\n",
        "latents = word2wave.latents.detach().cpu().numpy()\n",
        "\n",
        "if save_output:\n",
        "  np.save(os.path.join(save_dir, \"audio/{}.npy\".format(text)), generated_audio)\n",
        "  np.save(os.path.join(save_dir, \"latents/{}.npy\".format(text)), latents)\n",
        "\n",
        "ipd.Audio(generated_audio, rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azrsQs5m4ZCD",
        "cellView": "form"
      },
      "source": [
        "#@title Interpolate between samples { run: \"auto\" }\n",
        "\n",
        "source_name = \"firework\" #@param {type:\"string\"}\n",
        "target_name = \"wobble\" #@param {type:\"string\"}\n",
        "\n",
        "source_z = torch.tensor(np.load(\"/content/output/latents/{}.npy\".format(source_name)))\n",
        "target_z = torch.tensor(np.load(\"/content/output/latents/{}.npy\".format(target_name)))\n",
        "\n",
        "interpolation = latent_space_interpolation(word2wave.generator, n_samples=20, source=source_z, target=target_z)\n",
        "interpolation = interpolation.squeeze(1).flatten().detach().cpu().numpy()\n",
        "ipd.Audio(interpolation, rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA3zMi7FExc7",
        "cellView": "form"
      },
      "source": [
        "#@title Soundtrack your story (WIP) { run: \"auto\" }\n",
        "\n",
        "#@markdown Type some text in here, the model will check whether it recognises any of the words (it might not recognise any, its vocabulary is super small!), interpret them and turn them into sounds. It will then play back the final piece \"soundtracking\" your story, joke, random text. This uses latent space interpolation like the cell above, but with more samples. (Warning: it'll probably sound weird ðŸ¤·)\n",
        "\n",
        "text = \"Once upon a time...\" #@param {type:\"string\"}\n",
        "\n",
        "# concatenate all audio \n",
        "\n",
        "for i in text:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}